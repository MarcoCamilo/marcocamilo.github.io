[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "NLP\n\n\n\n\n\nBERT, Encoders and Linear Models for Resume Text Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\nAirline On-Time EDA\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore a data-driven journey into predicting and mitigating customer churn with this project on predictive analytics. Leveraging a Random Forest Classifier, discover insights that empower businesses to proactively retain customers and enhance overall satisfaction.\n\n\n\n\n\nBiodiversity, Endangerement and Conversation in Data from National Parks Service\n\n\n\n\n\n\n\n\n\n\n\n\nEmbark on a captivating exploration of biodiversity with this data science project, delving into the conservation statuses of endangered species across national parks. Through meticulous analysis, uncover profound insights into the distribution of endangered species, their likelihood of endangerment, and the most frequently spotted species in each park, illuminating the intricate dynamics of wildlife preservation and ecological sustainability.\n\n\n\n\nMachine Learning\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi, I’m Marco Camilo, \n",
    "section": "",
    "text": "Hi, I’m Marco Camilo, \n\nI design, build, and operate deep learning systems that process natural language. I also specialize in data analysis and machine learning. Here are some of the projects in my portfolio:\nBERT, Encoders and Linear Models for Resume Text Classification\nExploring performance of advanced NLP techniques on topic modeling, text classification and document similarity. 11 Jun 2024 • GitHub • Jupyter • Article\nAirline On-Time Performance across 80 Years EDA\nAn analysis of airline peformance from 2 million flights spanning 80 years. 30 Apr 2024 • GitHub • Jupyter • Article\nRaspberry-LLM - Making My Raspberry Pico a Little Smarter\nGenerating Dr. Seuss headlines, fake WSJ quotes, HackerNews troll comments, and more. 16 Apr 2024 • GitHub • Jupyter • Article\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "projects/resume-analyzer.html#executive-summary",
    "href": "projects/resume-analyzer.html#executive-summary",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe project explores the performance of advanced NLP models and vectorization techniques specifically for text classification using a dataset of resumes from various job categories. The algorithms tested include Linear SVC, Feedforward Neural Networks (FNN), Encoder models, and BERT, implemented using Scikit-Learn and PyTorch. The project aims to compare the performance of these algorithms on text classification and evaluate their effectiveness in analyzing resume data.\n\nKey Findings\n\nKey Factors in Model Performance: quality of the feature representations and the ability to capture contextual information across dependencies.\nModel performance\n\nBERT: Best performing model with an accuracy of 91.67%. Showcases the effectiveness of pre-trained models and transfer learning.\nLinear SVC: Achieved an accuracy of 87.15% with TF-IDF vectors. Attributed to the model’s simplicity and use of effective feature representation.\nEncoder Model: Achieved an accuracy of 74.54%. Suggests the need for further fine-tunning, give the difference with its cousin transformer model.\nFeedforward Neural Network: achieved an accuracy of 73.15%, the lowest of the four models. Indicates struggle with sequential dependencies and contextual nuances.\n\nPreprocessing and Vectorization: effective text preprocessing and robust vectorization techniques significantly enhanced overall model performance (&gt; 70%)."
  },
  {
    "objectID": "projects/resume-analyzer.html#dataset",
    "href": "projects/resume-analyzer.html#dataset",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Dataset",
    "text": "Dataset\nThe project primarily makes use of the Resume Dataset sourced from LiveCareer, available at Kaggle. The dataset consists of a collection of more than 2400 resumes in both string and HTML format, along with their respective labeled categories. The dataset consists of the following variables:\n\n\n\n\nID: Unique identifier for each resume\nResume_str: Text content of the resume\nResume_html: HTML content of the resume\nCategory: Categorized field of the resume (e.g. Information-Technology, Teacher, Advocate, Business-Development, Healthcare)"
  },
  {
    "objectID": "projects/resume-analyzer.html#preprocessing",
    "href": "projects/resume-analyzer.html#preprocessing",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Preprocessing",
    "text": "Preprocessing\nData preprocessing primarily involved text preprocessing, as well as data rebalancing for the categories in the resume dataset.\nFor text cleaning, I used a custom preprocessing function that streamlines multiple text cleaning and preprocessing operations. The function is highly flexible, offering tunable parameters to adapt the preprocessing pipeline to include key operations such as lowercase conversion, HTML decoding, email and URL removal, special character removal, contraction expansion, custom regex cleaning, as well as tokenization, stemming, lemmatization, and stopwords removal.\nIn particular, I enhanced the text preprocessing by removing noise from the text data, such as non-existen words and frequent, document-specific stopwords like months, section headings and other common words in resumes.\n\n\nView Code\nimport re\nfrom bs4 import BeautifulSoup\nfrom unidecode import unidecode\nimport contractions\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef preprocessing(\n    text, \n    tokenize=False,\n    stem=False,\n    lem=False,\n    html=False,\n    exist=False,\n    remove_emails=True,\n    remove_urls=True,\n    remove_digits=True,\n    remove_punct=True,\n    expand_contractions=True,\n    remove_special_chars=True,\n    remove_stopwords=True,\n    lst_stopwords=None,\n    lst_regex=None\n) -&gt; str | list[str]:\n    \n    # Lowercase conversion\n    cleaned_text = text.lower()\n\n    # HTML decoding\n    if html:\n        soup = BeautifulSoup(cleaned_text, \"html.parser\")\n        cleaned_text = soup.get_text()\n    \n    # Remove Emails\n    if remove_emails:\n        cleaned_text = re.sub(r\"([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)\", \" \", cleaned_text)\n    \n    # URL removal\n    if remove_urls:\n        cleaned_text = re.sub(r\"(http|https|ftp|ssh)://[\\w_-]+(?:\\.[\\w_-]+)+[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-]?\", \" \", cleaned_text)\n    \n    # Remove escape sequences and special characters\n    if remove_special_chars:\n        cleaned_text = re.sub(r\"[^\\x00-\\x7f]\", \" \", cleaned_text)\n        cleaned_text = unidecode(cleaned_text)\n    \n    # Remove multiple characters\n    cleaned_text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", cleaned_text)\n    \n    # Expand contractions\n    if expand_contractions:\n        cleaned_text = contractions.fix(cleaned_text)\n        cleaned_text = re.sub(\"'(?=[Ss])\", \"\", cleaned_text)\n    \n    # Remove digits\n    if remove_digits:\n        cleaned_text = re.sub(r\"\\d\", \" \", cleaned_text)\n    \n    # Punctuation removal\n    if remove_punct:\n        cleaned_text = re.sub(\"[!\\\"#$%&\\\\'()*+\\,-./:;&lt;=&gt;?@\\[\\]\\^_`{|}~]\", \" \", cleaned_text)\n    \n    # Line break and tab removal\n    cleaned_text = re.sub(r\"[\\n\\t]\", \" \", cleaned_text)\n    \n    # Excessive spacing removal\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n    \n    # Regex (in case, before cleaning)\n    if lst_regex: \n        for regex in lst_regex:\n            compiled_regex = re.compile(regex)\n            cleaned_text = re.sub(compiled_regex, '', cleaned_text)\n\n    # Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)\n    if stem or lem or remove_stopwords or tokenize:\n        if isinstance(cleaned_text, str):\n            cleaned_text = cleaned_text.split()\n        \n        # Remove stopwords\n        if remove_stopwords:\n            if lst_stopwords is None:\n                lst_stopwords = set(stopwords.words('english'))\n            cleaned_text = [word for word in cleaned_text if word not in lst_stopwords]\n\n        # Remove non-existent words\n        if exist:\n            english_words = set(words.words())\n            cleaned_text = [word for word in cleaned_text if word in english_words]\n\n        # Stemming\n        if stem:\n            stemmer = PorterStemmer()\n            cleaned_text = [stemmer.stem(word) for word in cleaned_text]\n\n        # Lemmatization\n        if lem:\n            lemmatizer = WordNetLemmatizer()\n            cleaned_text = [lemmatizer.lemmatize(word) for word in cleaned_text]\n        \n        if not tokenize:\n            cleaned_text = ' '.join(cleaned_text)\n\n    return cleaned_text\n\n\nNext, I prepared the Category variable in the resumes dataset by converting it to numerical representation using LabelEncoder from Scikit-Learn, as these algorithms require numerical feature representations. After cleaning, I saved the text separately for topic modeling and document similarity tasks, where dataset balancing is unnecessary.\nIn particular for text classification, I addressed the imbalance in underrepresented categories by employing random resampling with Scikit-Learn’s resample method. This approach ensures that the model receives equitable representation across all categories, leading to enhanced accuracy and reduced bias, thereby improving overall performance."
  },
  {
    "objectID": "projects/resume-analyzer.html#linear-svc",
    "href": "projects/resume-analyzer.html#linear-svc",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Linear SVC",
    "text": "Linear SVC\n\n\n\n1 Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the maximum-margin hyperplane, that is, the hyperplane that most clearly classifies observations2 Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.For this first model, I train a baseline Linear SVC1 using the TF-IDF vectors. I then performance Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)2 to the TF-IDF matrix, to reduce the matrix size to a lower dimensional space. I evaluate the performance of both models, which will serve as the baseline to compare with more complex models in the upcoming sections.\n\nTakeaways\n\nLinear SVC achieved an accuracy of 84% on the test set.\nLinear SVC with Truncated SVD achieved an accuracy of 81% on the test set with only 5% of the original number of features.\nThe baseline model achieved a high level of accuracy and is more cost-effective when reducing the dimensionality of the feature matrix.\n\n\n\nImport Packages and Data\nAside from standard libraries, I import two custom functions: classifier_report to generate a classification report and confusion matrix, and save_performance, which saves the performance metrics of the model to a JSON file for later analysis. I also load the pre-trained Label Encoder to label the encoded categories in upcoming plots.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import TruncatedSVD\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom src.modules.ml import classifier_report\nfrom src.modules.utils import save_performance\n\n# Import the data with engineered features\ndf = pd.read_parquet('./data/3-processed/resume-features.parquet')\n\n# Label model to label categories\nle = joblib.load('./models/le-resumes.gz')\n\n\n\nBaseline LinearSVC\nI split the dataset into 70% training and 30% testing sets. I use the tfidf_vectors column as the feature matrix and stack the vectors into a single matrix using np.vstack. For the target variable I set the encoded Category column. I extract the variables using the .values method, which extracts the variables into a NumPy array, improving performance. To verify the split, I print the shape of the training and testing sets.\n\n\nView Code\nX = np.vstack(df['tfidf_vectors'].values)\ny = df['Category'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n\n\nTraining set: (2016, 11618), (2016,)\nTesting set: (864, 11618), (864,)\nThen, I train a Linear SVC model with default hyperparameters and evaluate the model’s performance using the classifier_report function, which generates a classification report and confusion matrix.\n\n\nView Code\nsvc = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, True)\n\n\nSVC accruacy score 87.15%\n\nThe model achieves an 87% accuracy on the test set, which is already very good for a baseline model. However, TF-IDF vectors often result in sparse, high-dimensional representations with a low information ratio. To address this issue, I recur to Truncated Singular Value Decomposition (SVD).\n\n\nLinearSVC with Truncated SVD\n\n\n\n3 For an in depth explanation, see Manning, C.D., Raghavan, P. and Schütze, H. (2008) ‘Matrix decompositions and latent semantic indexing’, Introduction to information retrieval, 1.Transforming TF-IDF matrices by means of Truncated SVD is known as Latent Semantic Analysis (LSA). It takes the \\(n\\) largest eigenvalues and transforms the original matrix to capture the most significant semantic relationships between terms and documents, while discarding noise and low-information features3.\nI use TruncatedSVD to reduce the number of components to 500, which surprisingly is less than 5% of the original number of features. Below I apply the transformation and split the transformed feature matrix into training and testing sets before training the new model.\n\n\nView Code\nt_svd = TruncatedSVD(n_components=500, algorithm='arpack')\nX_svd = t_svd.fit_transform(X)\n\n\nX_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(X_svd, \n                                                                    y, \n                                                                    test_size=0.30, \n                                                                    random_state=42)\n\n\nprint(f\"Training set: {X_train_svd.shape}, {y_train_svd.shape}\")\nprint(f\"Testing set: {X_test_svd.shape}, {y_test_svd.shape}\")\n\n\nTraining set: (2016, 500), (2016,)\nTesting set: (864, 500), (864,)\nI now train a new Linear SVC model using the SVD-transformed feature matrix and generate a classification report and confusion matrix.\n\n\nView Code\nsvc_svd = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, True)\n\n\nSVC accruacy score 84.94%\n\nThe model achieved an 84% accuracy on the test set, which is slightly lower than the baseline model. However, these results come from a model trained on less than 5% of the original number of features while still retaining a high level of accuracy. This demonstrates the sparcity of the TF-IDF vectors, while also showing that the model can still achieve an excellent level of accuracy with a substantially reduced feature matrix.\nBefore moving onto the next model, I save the performance metrics of the baseline model for later comparison.\n\n\nView Code\nsave_performance(model_name='LinearSVC',\n                 architecture='default',\n                 embed_size='n/a',\n                 learning_rate='n/a',\n                 epochs='n/a',\n                 optimizer='n/a',\n                 criterion='n/a',\n                 accuracy=87.15\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#feedforward-neural-network",
    "href": "projects/resume-analyzer.html#feedforward-neural-network",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\n\n\n\n4 Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodesThe next model is a Feedforward Neural Network (FNN)4 built using PyTorch. I create an iterator for the dataset using the DataLoader class, which tokenizes and numericalized the resumes, dynamically pads the sequences, and batches the data for training, saving memory and computation time. Then, I construct a simple neural network architecture with an embedding layer, followed by three fully connected layers with ReLU activation functions. The model is trained using the Adam optimizer and CrossEntropyLoss criterion, and its performance is evaluated on the test set using accuracy as the evaluation metric.\n\nTakeaways\n\nThe Feedforward Neural Network achieved an accuracy of 73.15% with a loss of 1.1444 on the test set.\nPerformance suggests minimal overfitting, given the small gap between training and validation.\nModel demonstrates robust generalization, with test accuracy aligning closely with validation accuracy.\n\n\n\nImport Packages and Data\nAside from the standard PyTorch and pandas imports, I also import three custom functions:\n\ntrain_model: trains the model based on the hyperparameters and data provided, prints the training and validation loss and accuracy in real-time, and saves the best model based on the iteration with the lowest validation loss. It also provides an option to visualize the training progress using the PlotLosses library or matplotlib.\ntest_model: evaluates the model on the test set using the best model saved during training and returns the testing accuracy.\nsave_performance: saves the performance metrics of the model to a json file for future analysis.\n\n\n\nView Code\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom livelossplot import PlotLosses\nfrom tqdm import tqdm\n\nfrom src.modules.dl import train_model, test_model\nfrom src.modules.utils import save_performance\n\ndata = pd.read_parquet('./data/3-processed/resume-features.parquet', columns=['Category', 'cleaned_resumes'])\n\n\nWhen training deep learning models, I always code the option to use a GPU if available and set the device variable accordingly. This not only allows the model to leverage the parallel computing if available, but also makes the code reproducible across different device setups. The snippet below checks if a GPU is available and sets the device variable accordingly, which is later used by the DataLoader and model.\n\n\nView Code\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device('cpu')\nprint(\"Using {}.\".format(device))\n\n\nUsing cpu.\n\n\nDataset and DataLoader\nBefore constructing the Dataset class, I define a tokenization function that instantiates the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s get_tokenizer and build_vocab_from_iterator functions. The function returns the tokenized texts to be indexed by the DataLoader during training, and the vocabulary, which will be used to determine the vocabulary size.\n\n\nView Code\ndef tokenization(texts, tokenizer_type='basic_english', specials=['&lt;unk&gt;'], device=device):\n    # Instantiate tokenizer\n    tokenizer = get_tokenizer(tokenizer_type)\n    # Tokenize text data\n    tokens = [tokenizer(text) for text in texts]\n    # Build vocabulary\n    vocab = build_vocab_from_iterator(tokens, specials=specials)\n    # Set default index for unknown tokens\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n\n    # Convert tokenized texts to a tensor\n    tokenized_texts = [torch.tensor([vocab[token] for token in text], dtype=torch.int64, device=device) for text in tokens]\n\n    return tokenized_texts, vocab\n\n\nNext. I construct the ResumeDataset iterator, which preprocesses the text data using the tokenization function and indexes samples for the DataLoader during training. The __len__ method returns the length of the dataset, the vocab_size method returns the size of the vocabulary, the num_class method returns the number of unique classes in the dataset, and the __getitem__ method returns a sample of text and label from the dataset.\n\n\nView Code\nclass ResumeDataset(Dataset):\n    # Dataset initialization and preprocessing\n    def __init__(self, data):\n        # Initialize dataset attributes\n        super().__init__()\n        self.text = data.iloc[:,1]\n        self.labels = data.iloc[:,0]\n        \n        self.tokenized_texts, self.vocab = tokenization(self.text)\n\n    # Get length of dataset\n    def __len__(self):\n        return len(self.labels)\n\n    # Get vocabulary size\n    def vocab_size(self):\n        return len(self.vocab)\n\n    # Get number of classes\n    def num_class(self):\n        return len(self.labels.unique())\n\n    # Get item from dataset\n    def __getitem__(self, idx):\n        sequence = self.tokenized_texts[idx]\n        label = self.labels[idx]\n        return sequence, label\n\n\nI also define a collate_fn function to use dynamic padding when batching the data. Dynamic padding is a technique used to pad sequences to the length of the longest sequence in a batch, as opposed to the longest sequence in the entire dataset. Because the model expects uniform dimensions to perform operations, sequences need to be padded to ensure each one has the same length. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.\n\n\nView Code\ndef collate_fn(batch):\n    sequences, labels = zip(*batch)\n    # Pad sequences to the longest sequence in the batch\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    # Convert labels to tensor\n    labels = torch.tensor(labels, dtype=torch.long)\n    return sequences_padded, labels\n\n\nFinally, I instantiate the ResumeDataset class and split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. I create DataLoader iterators for each set, using the collate_fn function to apply dynamic padding to the sequences.\n\n\nView Code\ndataset = ResumeDataset(data)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\n\n\n\nModel Architecture\n\n\n\nThe model is a simple Feedforward Neural Network with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with the number of classes as the output size. The model architecture is defined in the SimpleNN class, which takes the vocabulary size, embedding size, number of classes as parameters. The expansion_factor is defined to determine the hidden dimension size, here set to 2.\nThe EmbeddingBag function efficiently computes the embeddings by performing a two-step operation: first, it creates embeddings for the input indices, adn then reduces the embedding output using the mean across the sequence dimension. This is useful for sequences of varying lengths, as it allows the model to process them more efficiently.\n\n\nView Code\nclass SimpleNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_class, expansion_factor=2, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_size, sparse=False)\n        self.hidden_dim = embed_size * expansion_factor\n        self.layer1 = nn.Linear(embed_size, self.hidden_dim)\n        self.layer2 = nn.Linear(self.hidden_dim, embed_size)\n        self.layer3 = nn.Linear(embed_size, num_class)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.relu(self.layer1(x))\n        x = self.dropout(x)\n        x = F.relu(self.layer2(x))\n        x = self.dropout(x)\n        x = self.layer3(x)\n        return x\n\n\n\n\nHyperparameters and Training\nBefore training, I set the hyperparameters for training the neural network. The vocabulary size and number of classes are obtained from the ResumeDataset class.The embedding size is set to 60 and the learning rate is set to 1e-3. The model is trained for 40 epochs.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nnum_class = dataset.num_class()\nembed_size = 60\nlr=0.001\nepochs = 40\n\n\nI then instantiate the model, sending it to the available device, and define the loss function and optimizer. The loss function is set to CrossEntropyLoss, which is suitable for multi-class classification tasks. The optimizer is Adam, an adaptive learning rate optimization algorithm well-suited for training deep neural networks. In addition, I define a learning rate scheduler that reduces the learning rate by a factor of 0.1 if the validation loss does not improve for patience number of epochs. This prevents the model from overfitting and improves generalization. The model and hyperparameters are then passed for training using the train_model function5.\n5 During fine-tunning, I found the model converged with better accuracy when using a dropout rate of 0.4.To visualize the training progress, I set the visualize parameter to ‘liveloss’, which uses the PlotLosses library to create a dynamicallly updating plot that visulalized the training and validation loss and accuracy in real-time. This allows me to monitor the model’s performance and make adjustments to the hyperparameters if necessary.\n\n\nView Code\nmodel = SimpleNN(vocab_size, embed_size, num_class, dropout=0.4).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize='liveloss')\n\n\n\naccuracy\n  training           (min:    0.036, max:    0.712, cur:    0.707)\n  validation         (min:    0.037, max:    0.694, cur:    0.685)\nlog loss\n  training           (min:    0.873, max:    3.187, cur:    0.921)\n  validation         (min:    1.274, max:    3.184, cur:    1.330)\n------------------------------\nBest model saved:\nVal Loss: 1.3300 | Val Acc: 0.6852\n✅ Training complete!\nStarting with the loss, the training and validation losses decrease steadily until 30 epochs, showing that the model is effectively learning the patterns in the data. After this, the training loss continues to decrease, while the validation loss plateaus. However, the model finishes with only a small gap between the training and validation loss, with the training loss at 0.92 and the validation loss at 1.33. The small size of the gap indicates that the model is not overfitting the training data and generalizes well to unseen data.\nRegarding accuracy, the training and validation accuracy increase steadily until 40 epochs, after which they converge. At convergence, the training accuracy is slightly higher than the validation accuracy, with the model achieving a training accuracy of 71% and a validation accuracy of 69%. This small difference between the training and validation accuracy indicates that the model does not overfit the training data and generalizes well to unseen data. The best saved model has a validation loss of 1.33 and a validation accuracy of 68.52%.\n\n\nEvaluation\nAfter training the model, I evaluate its performance on the test set using the test_model function. The function takes the trained model, test data loader, and criterion as input and returns the accuracy of the model on the test set.\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.1444 | Test Acc: 0.7315\n✅ Testing complete!\nThe Feedforward Neural Network model achieves an accuracy of 73.15% and a loss of 1.1444 on the test set. As expected from the training and validation plots, the model performs reasonably well on unseen data, with the test accuracy aligning closely with the validation accuracy observed during training. The consistency between validation and test accuracies suggests that the model successfully generalizes to new data and demonstrates robustness in its predictions.\nCompared to the baseline model, the Feedforward Neural Network model achieved a lower accuracy on the test set. However, the model’s performance is still quite impressive considering the simplicity of the architecture. The model’s performance can likely be improved by introducing more advanced regularization techniques, using pre-trained word embeddings6, or increasing the complexity of the model architecture. This last point is what I explore in the next section, where I implement a more advanced model architecture using a Transformer-based neural network that leverages self-attention mechanisms to capture long-range dependencies in the data.\n6 Examples of pre-trained embeddings include: Word2Vec, GloVe, and fastText.To conclude this section, I save the performance metrics of the Feedforward Neural Network model for later analysis.\n\n\nView Code\nsave_performance(model_name='Feedforward Neural Network',\n                 architecture='embed_layer-&gt;dropout-&gt;120-&gt;dropout-&gt;60-&gt;dropout-&gt;num_classes',\n                 embed_size='60',\n                 learning_rate='1e-3',\n                 epochs='50',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=73.15,\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#encoder-model",
    "href": "projects/resume-analyzer.html#encoder-model",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Encoder Model",
    "text": "Encoder Model\n\n\n\nThe following model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which convert dense representations into output sequences, encoders instead transform input sequences into dense representations. Their ability to extract sequence information and convert them to a dense representation makes them particularly useful for tasks such as sentiment analysis, named entity recognition, and text classification.\nThe encoder model follows the Transformer architecture described in Attention is All You Need7 and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and CrossEntropyLoss criterion. The model’s performance is evaluated on the test set using accuracy as the evaluation metric.\n7 Attention Is All You NeedThe imported packages as well as the step in building the DataLoader are the same as those for the Feedforward Neural Network model (see packages and data preparation. Therefore, I skip directly to the model architecture.\n\nTakeaways\n\nThe Transformer Encoder model achieves an accuracy of 75% on the test set.\nThis model serves as a robust baseline for transformer-based models in text classification tasks.\n\n\n\nModel Architecture\n\n\n\n\n\nModified image from Sebastian Raschka\n\n\n8 The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.9 Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.The Encoder model consists of an embedding layer, a stack of encoder layers, and a fully connected neural network for classifying the outputs. The embedding layer converts the input sequences into dense representations, which are then passed through the encoder layers. Each encoder layer consists of a multi-head self-attention mechanism8 followed by a residual connection with layer normalization9 and a feed-forward neural network, followed by another residual connection with layer normalization. The output is finally passed through a feed-forward neural network for classification.\nI decide to build the encoder model from scratch, as it allows me to better understand the architecture and the components of the model. I opt for a modular approach, where I construct each component of the model as a separate class and then combine them in the TransformerEncoder class.\n\nFurther Reading:\nThe implementation of this model was in great part inspired by the following resources: - The Annotated Transformer - Coding a Transformer from Scratch on PyTorch (YouTube) - Text Classification with Transformer Encoders\n\n\n\nView Code\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        # Dimensions of embedding layer\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        # Embedding dimension\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        # Initialize positional embedding matrix (vocab_size, d_model)\n        pe = torch.zeros(vocab_size, d_model)\n        # Positional vector (vocab_size, 1)\n        position = torch.arange(0, vocab_size).unsqueeze(1)\n        # Frequency term\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))\n        # Sinusoidal functions\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add batch dimension\n        pe = pe.unsqueeze(0)\n        # Save to class\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        # Learnable parameters\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.ones(d_model))\n        # Numerical stability in case of 0 denominator\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        # Linear combination of layer norm with parameters gamma and beta\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        # Layer normalization for residual connection\n        self.norm = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x1, x2):\n        return self.dropout(self.norm(x1 + x2))\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):\n        super().__init__()\n        # Linear layers and dropout\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float =0.1, qkv_bias: bool = False, is_causal: bool = False):\n        super().__init__()\n        assert d_model % num_heads == 0,  \"d_model is not divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.dropout = dropout\n        self.is_causal = is_causal\n\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)\n        self.linear = nn.Linear(num_heads * self.head_dim, d_model)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_length = x.shape[:2]\n\n        # Linear transformation and split into query, key, and value\n        qkv = self.qkv(x)  # (batch_size, seq_length, 3 * embed_dim)\n        qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)  # (batch_size, seq_length, 3, num_heads, head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_length, head_dim)\n        queries, keys, values = qkv  # 3 * (batch_size, num_heads, seq_length, head_dim)\n\n        # Scaled Dot-Product Attention\n        context_vec = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask, dropout_p=self.dropout, is_causal=self.is_causal)\n\n        # Combine heads, where self.d_model = self.num_heads * self.head_dim\n        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        context_vec = self.dropout_layer(self.linear(context_vec))\n\n        return context_vec\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        # Multi-head self-attention mechanism\n        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)\n        # First residual connection and layer normalization\n        self.residual1 = ResidualConnection(d_model, dropout)\n        # Feed-forward neural network\n        self.feed_forward = FeedForward(d_model, hidden_dim, dropout)\n        # Second residual connection and layer normalization\n        self.residual2 = ResidualConnection(d_model, dropout)\n\n    def forward(self, x, mask=None):\n        x = self.residual1(x, self.multihead_attention(x, mask))\n        x = self.residual2(x, self.feed_forward(x))\n        return x\n\nclass EncoderStack(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, dropout: float = 0.1):\n        super().__init__()\n        # Stack of encoder layers\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, out_features: int, dropout: float = 0.1):\n        super().__init__()\n        self.embedding = EmbeddingLayer(vocab_size, d_model)\n        self.positional_embedding = PositionalEmbedding(vocab_size, d_model, dropout)\n        self.encoder = EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)\n        self.classifier = nn.Linear(d_model, out_features)\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.positional_embedding(x)\n        x = self.encoder(x, mask)\n        x = x.mean(dim=1)\n        x = self.classifier(x)\n        return x\n\n\n\n\nHyperparameters and Training\nWith the model constructed, I initialize the hyperparameters for training. As with the previous model, I obtain the vocabulary size and number of output features from the ResumeDataset class. The embedding size is fixed at 80, while the hidden dimension is set to 180. For the multi-head attention mechanism, I use 4 heads, with an the encoder stack comprising of 4 layers. I train the model for 20 epochs at a learning rate of 1e-3.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nd_model = 80\nnum_heads = 4\nhidden_dim = 180\nnum_layers = 4\nout_features = dataset.num_class()\nlr = 0.001\nepochs = 20\n\n\nI instantiate the model with the hyperparameters and move it to the device. The criterion and optimizer are left unchanged from the previous model, with the optimizer set to the Adam optimizer and the criterion set to the CrossEntropyLoss, suitable for multi-class classification tasks. I also initialize the learning rate scheduler with a patience of 2, to prevent the model from overfitting. The model is then trained using the train_model function.\n\n\nView Code\nmodel = TransformerClassifier(vocab_size, d_model, num_heads, \n                                hidden_dim, num_layers, out_features, dropout=0).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)\n\n\n\naccuracy\n  training           (min:    0.043, max:    1.000, cur:    1.000)\n  validation         (min:    0.035, max:    0.794, cur:    0.785)\nlog loss\n  training           (min:    0.027, max:    3.276, cur:    0.029)\n  validation         (min:    1.023, max:    3.273, cur:    1.071)\n------------------------------\nBest model saved:\nVal Loss: 1.0709 | Val Acc: 0.7847\n✅ Training complete!\nAccording to the performance plots, despite the encoder model achieving a better validation accuracy than the Feedforward Neural Network, the model exhibits a large gap between the training and validation performance, indicating that the model is overfitting. The model rapidly decreases the training and validation losses, and rapidly incrases their accuracies during the first 14 epochs. For the following 6 epochs, however, the training loss continues to rapidly decrease from 1.25 to near 0, while the validation loss stagnates at around 1.07. Similarly, the training accuracy rapidly increases to 100%, while the validation accuracy remains at 78%. The model converges 20 epochs in with a significant gap between training and validation models, indicating that the model is overfitting.\nNevertheless, the model achieves a validation accuracy of 78%, which is slightly higher than the Feedforward Neural Network model. I decide to evaluate the model on the test set to obtain the final accuracy.\n\n\nEvaluation\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.2526 | Test Acc: 0.7454\n✅ Testing complete!\nDespite having implemented a more advanced model architecture with the addition of the multi-head self-attention mechanism, the encoder model achieves an accuracy of 74.5%, similar to the Feedforward Neural Network model. As discussed earlier, the model exhibits a large gap between the training and validation performance, which could be hindering the model’s generalization capabilities.\nGiven a better training and validation performance, the model could potentially achieve a higher accuracy on the test set. The model’s performance could likely be improved by means of data augmentation, modifying hyperparameters such as embedding size, hidden dimension, and number of layers, or by using a different optimizer or learning rate scheduler.\nAs with the previous models, I save the performance metrics for later analysis.\n\n\nView Code\nsave_performance(model_name='Transformer',\n                 architecture='embed_layer-&gt;encoder-&gt;linear_layer',\n                 embed_size='64',\n                 learning_rate='1e-3',\n                 epochs='20',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=80\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#bert",
    "href": "projects/resume-analyzer.html#bert",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "BERT",
    "text": "BERT\n\n\n\nThe last model is also an encoder-based model called the Bidirectional Encoder Representations from Transformers (BERT). BERT is a pre-trained transformer-based model that can be fine-tuned for a wide range of NLP tasks by adding task-specific output layers. The model is bidirectional, meaning that it can take into account the context of a word by looking at both the left and right context. This allows the model to capture a wider range of contextual information, which is particularly useful for tasks such as text classification. This allows the model to capture rich semantic relationships and dependencies within text sequences, which is particularly useful for tasks such as text classification.\nAs with previous PyTorch models, I create an iterable dataset using the Dataset and DataLoader classes, to tokenize the resumes using the BERT tokenizer, pad sequences to equal lengths, split the data and batch the data for training. I then construct the model architecture, consisting of the pre-trained BERT base model, an added dropout layer and a linear output layer for classification. I initialize the hyperparameters and train the model using Cross Entropy Loss along with the Adam optimizer. The model is evaluated as with other deep learning models using accuracy.\n\n### Takeaways\n\n\n\n\n\n\nImport Packages\nIn addition to the standard deep learning packages used so far, I import three classes from the transformers package:\n\nBertModel: loads the pre-trained BERT model.\nBertTokenizer: constructs a BERT tokenizer.\nDataCollatorWithPadding: builds a batch with dynamically padded sequences.\n\nBecause of BERT’s output format, I also import a custom train_BERT and test_BERT function, which are specifically tailored to return the model’s training and test performances using BERT’s output, including input IDs and attention masks.\n\n\nView Code\nfrom transformers import BertModel, BertTokenizer, DataCollatorWithPadding\n\nfrom src.modules.dl import train_BERT, test_BERT\n\n\n\n\nDataset and DataLoader\nBefore creating the dataset and dataloader, I initizalize the tokenizer and define the pre-trained BERT model. I then create the ResumeBertDataset, which tokenizes resumes and prepares them for model input.\nIn contrast to the previous model, I configure the tokenizer using the .encode_plus method, which returns a dictionary of the batch encodings, including tokenized input sequences and attention masks. I set the padding parameter to False to avoid padding, as this will be handled dynamically by the data collator. Additionally, I set truncation to True to truncate sequences that exceed the maximum length. The method also adds the special tokens [CLS] and [SEP] to the input sequences, required by BERT. I set the return_tensors parameter to 'pt' to return PyTorch tensors. Finally, I return a dictionary with the processed input sequences, attention masks, and labels.\n\n\nView Code\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass ResumeBertDataset(Dataset):\n    def __init__(self, data, max_length, tokenizer=tokenizer, device=device):\n        super().__init__()\n        self.texts = data.iloc[:,1].values\n        self.labels = torch.tensor(data.iloc[:,0])\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def num_class(self):\n        return len(self.labels.unique())\n\n    def __getitem__(self, idx):\n        resumes = self.texts[idx]\n        labels = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            resumes,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation=True,\n            padding=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ).to(self.device)\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n\nI initialize the dataset and set max_length to 512, which is the maximum number of tokens that BERT can process. I then split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. I use the DataCollatorWithPadding class to dynamically pad sequences to the maximum length in each batch. Finally, I create dataloaders for the training, validation, and test sets using the DataLoader class, setting the batch size to 16, shuffling the data, and assigning the data collator.\n\n\nView Code\ndataset = ResumeBertDataset(data, max_length=512)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n\n\n\n\nModel Architecture\nThe BertResumeClassifier model consists of the pre-trained BERT base model, a dropout layer, and a linear output layer to classify the resumes. The BERT model uses the bert-base-uncased pre-trained model to generate contextual embeddings from the input sequences. I extract the embeddings by indexing the pooler_output key from the output dictionary. These embeddings are then passed through a dropout layer to prevent overfitting, and subsequently fed into a fully connected linear layer which maps the embeddings to the desired number of output classes for classification.\n\n\nView Code\nclass BertResumeClassifier(nn.Module):\n    def __init__(self, n_classes: int, dropout: float = 0.01):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )['pooler_output']\n        output = self.dropout(pooled_output)\n        output = self.out(output)\n        return output\n\n\n\n\nHyperparameters and Training\nSince BERT is a pre-trained model and it accepts a fixed input size of 512 tokens, the are less parameters to set from the model itself. The only parameter that needs to be set is the number of classes—which, as before, is obtained from the Dataset class.\nI initialize the model, loss function, optimizer, and number of epochs. I use as before the Cross Entropy Loss function and the Adam optimizer, although this time with a learning rate of 2e-5, since it seemed to result in better performance. I train the model for only 10 epochs.\n\n\nView Code\nn_classes = dataset.num_class()\n\nmodel = BertResumeClassifier(n_classes).to(device)\ncriterion = CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=2e-5)\nepochs = 10\n\ntrain_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)\n\n\n\naccuracy\n    training             (min:    0.050, max:    0.991, cur:    0.991)\n    validation           (min:    0.120, max:    0.933, cur:    0.933)\nlog loss\n    training             (min:    0.081, max:    3.189, cur:    0.081)\n    validation           (min:    0.414, max:    3.002, cur:    0.428)\n------------------------------\nBest model saved:\nVal Loss: 0.4143 | Val Acc: 0.9190\n✅ Training complete!\nThe BERT model shows a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the model’s train loss decreased from 3.1394 to 0.0589, while the validation loss decreased from 2.7347 to 0.5070. This consistent reduction highlights the model’s ability to capture and generalize the data without overfitting, as demonstrated by the small gap between the training and validation losses by the end of the 10th epoch.\nRegarding accuracy, the training and validation accuracies also show a steady increase over the 10 epochs. The training accuracy increased from 0.0774 to 0.9950, with validation accuracy improving from 0.3472 to 0.9028. This indicates that the model effectively learned the patterns in the data and generalizes well to unseen data. The small difference between the final training and validation accuracies demonstrates the model’s robustness and ability to avoid overfitting, ensuring reliable performance on new data. The best saved model has a validation loss of 0.5070 and an accuracy of 0.9028.\n\n\nEvaluation\nAs before, I evaluate the model using the test_model function using the best saved model.\n\n\nView Code\naccuracy = test_BERT(model, test_loader, criterion)\n\n\nTest Loss: 0.3984 | Test Acc: 0.9167\n✅ Testing complete!\nThe BERT model achieved an impressive performance on the test set, reaching an accuracy of 91.67% with a test loss of 0.3984. This significantly outperforms all previous models tested so far. As expected from the training and validation performances, the model is robust and generalizes very well to unseen data. This is further confirmed by the very close alignement between test and validation accuracies, both of which represent datasets not previously seen by the model. The close accuracy between the validation data and the test data shows the model is capable to generalize to new resumes and effectively classify them into the correct categories.\nAs with previous models, I save its performance using the save_performance function.\n\n\nView Code\nsave_performance(model_name='BERT',\n                 architecture='bert-base-uncased&gt;dropout-&gt;linear_layer',\n                 embed_size='768',\n                 learning_rate='2e-5',\n                 epochs='10',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=90\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#results-and-discussion",
    "href": "projects/resume-analyzer.html#results-and-discussion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nIn this section of the project, I explored four different models for resume classification: Linear SVC, FNN, Transformer, and BERT. I evaluated the performance of each model using the accuracy metric and collected the results after each deployment. Below I plot the accuracy of each model and discuss the results.\n\n\nView Code\nevaluation_df = pd.read_json('./output/classifier_performance.json').sort_values(by='accuracy', ascending=False)\n\nax = sns.barplot(evaluation_df, x='model', y='accuracy', hue='model', palette='hls')\n[ax.bar_label(container, fmt=\"%0.2f%%\") for container in ax.containers]\nplt.show()\n\n\n\n\n\nAll four models performed well in classifying resumes, achieving accuracies above 70%. This success can be largely attributed to the effectiveness of the data preparation process, including text preprocessing, data balancing, and robust vectorization techniques. These preprocessing steps provided the models with high-quality input features that significantly enhanced their performance.\nUpon closer evaluation, the models can be grouped into two categories based on their performance. The first group, achieving around 90% accuracy, includes the Linear SVC and BERT models. The second group, with accuracies around 70%, includes the FNN and Transformer models. Interestingly, the two best-performing models feature both the simplest and most complex architectures respectively, while the models with the lowest performance have more complex architectures than the baseline model. I discuss the reasons behind these results below.\nLinear SVC’s high performance can be attributed its simplicity and the effective feature representation10. The model is a classical machine learning algorithm that uses a linear kernel and no deep learning, which yields a simple architecture that is easy to train. Additionally, the model was trained on TF-IDF vectors, which result in a matrix with simple, but interpretable and informative features. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model’s high accuracy and fast training times.\n10 To read more on the efficiency of linear classifiers in text classification, see Lin, Y.-C. et al. (2023) ‘Linear Classifier: An Often-Forgotten Baseline for Text Classification’.11 See Devlin, J. et al. (2018) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’.In contrast, BERT’s performance strems from its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information11. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Its suitability for Transfer Learning allows maximizing its pre-trained weights and easily fine-tuning on resume classification task, resulting in the high accuracy achieved.\nThe FNN and Transformer models, despite their increased complexity, achieved lower accuracies of around 70%. The Feedforward Neural Network, while more advanced than a linear model, lacks the ability to capture sequential dependencies and contextual nuances in the data, thus expected to perform worse than the transformer models.\nHowever, the Transformer model should have been able to capture the sequential dependencies in the resume data, given its state-of-the-art architecture and use of multi-head self-attention. But contrary to BERT, the Transformer used in this project was not pre-trained on a large corpus. This limited its ability to generate high-quality dense representations of the texts. Additionally, insufficient fine-tuning may have prevented the Transformer from reaching its full potential. Given the results of a similar, more complex model such as BERT, additional hyperparameter tuning and training time could improve its performance."
  },
  {
    "objectID": "projects/resume-analyzer.html#conclusion",
    "href": "projects/resume-analyzer.html#conclusion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I explored the task of resume classification using machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented four models: a Linear Support Vector Classifier, a Feedforward Neural Network, a Transformer model, and a BERT model. These models were trained and evaluated on the resume dataset, with performance compared based on accuracy.\nThe results were very insightful and provoked interesting observations on the role of model complexity and feature representation in achieving high performance. The BERT model achieved the highest accuracy of 91.67%, closely followed by the LinearSVC model at 87.15%. The Feedforward Neural Network and Transformer models achieved lower accuracies of 73.15% and 74.54%, respectively. BERT’s superior performance can be attributed to its pre-trained transformer architecture, which captures rich semantic relationships and dependencies within text sequences. The strong performance of the LinearSVC model can be attributed to its simplicity and efficiency in handling high-dimensional data, leveraging high-quality, interpretable feature representations such as TF-IDF vectors.\nTwo important observations arise from these results:\n\n\nState-of-the-art transformer models, combined with transfer learning from pre-trained models like BERT, yield the best performance.\nSimple models with high-quality, interpretable feature representations such as TF-IDF vectors, can also achieve high performance.\n\n\nThese contrasting observations indicate that model complexity alone does not guarantee high performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline.\n\nThe quality of the feature representation and the ability to capture contextual information across dependencies are more important factors in achieving high performance.\n\nUltimately, the best model depends on the requirements of the task at hand and the resources available for development. For tasks where high performance is critical and ample resources are available, using state-of-the-art transformer models such as BERT with transfer learning are recommended. For tasks prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors still offers a high-performance, resource-efficient alternative."
  },
  {
    "objectID": "projects/airline.html",
    "href": "projects/airline.html",
    "title": "Airline On-Time EDA",
    "section": "",
    "text": "Embark on a data-driven exploration into predicting and mitigating customer churn with our project on predictive analytics. By harnessing the power of a Random Forest Classifier, uncover valuable insights that enable businesses to preemptively address customer churn and elevate overall satisfaction. Join us on this journey as we delve into the realm of customer retention and optimization."
  },
  {
    "objectID": "projects/airline.html#importing-packages-and-data",
    "href": "projects/airline.html#importing-packages-and-data",
    "title": "Airline On-Time EDA",
    "section": "Importing Packages and Data",
    "text": "Importing Packages and Data\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nimport missingno as msno\n\nimport sys\nsys.path.append(\"./src/modules/\")\nfrom variables_03 import *\nfrom mac_functions import *\n\nfigsize = (14,8)\n\nplt.rcParams['figure.figsize'] = figsize\nsns.set(rc={'figure.figsize':figsize})\npd.set_option('display.max_columns', 200)\npd.options.display.float_format = '{:,.3f}'.format\n\ndf = pd.read_parquet('./data/interim/03-airline_2m.parquet')"
  },
  {
    "objectID": "projects/airline.html#time-variables",
    "href": "projects/airline.html#time-variables",
    "title": "Airline On-Time EDA",
    "section": "Time Variables",
    "text": "Time Variables\nWe decide to analyze the time variables (CRSDepTime, DepTime, CRSArrTime, ArrTime) separate from the delay variables, as they require different data cleaning tasks and provide deeper insights into the distribution of the data. We will start by analyzing the distribution of these variables and determine any potential issues that may require further processing.\n\nValue Counts\ntime_cols = ['CRSDepTime', 'DepTime', 'CRSArrTime', 'ArrTime']\ncolors = ['red', 'green', 'yellow', 'blue']\n\ntime_hist(df, time_cols, colors, 75, 'Distribution of CRS and True Depature and Arrivals')\ntime_hist(df, time_cols, colors, 100, 'Distribution of CRS and True Depature and Arrivals: Zoomed into Values 0-100', limit=500)\n\n\nThe histograms above reveal several insights about the nature of time variables in the dataset. Firstly, we see the distribution is not continuous, but discrete with regular gaps throughout the range of values. This is probably due to the HHMM format, which, representing time values, excludes all values between 60 and 99. This results in a discontinous range of values, which is not ideal for out analysis.\nMoreover, there is a markedly lower frequency of values below 500, showing that flights before 500 are less frequent. The second histogram zooms into this range. The plot not only demosntrates the progression in the number of red eye flights between midnight and 5am, but also shows more clearly that the distribution of values is regularly discontinuous at values between 60 and 99. This confirms that HHMM is not a suitable format for time operations, and is instead must be replaced by a datetime object. Moreover, late-night ArrTime values in particular seem to be the most frequent among the time variables.\nFinally, there is a markedly large number of 0s among time variables. This could suggest that missing values are expressed by means of a 0 value. This might represent a two-fold issue: 0 is representing missing values, while 2400 represents midnight in an invalid time format. This results in midnight values and missing values are undistinguishable in the dataset. This will require some processing before converting values to a datetime object.\nWe will now address these issues in the following sections.\n\n\nUnderstanding 0s, 2400s and NaNs\nWe will start by exploring the value counts of 0s, 2400s and missing values in the time variables. We will then decide how to handle these values.\npd.DataFrame({\n    '0': df[time_cols].eq(0).sum(),\n    'na': df[time_cols].isna().sum(),\n    '2400': df[time_cols].eq(2400).sum()\n})\n\n\n\n\n0\nna\n2400\n\n\n\n\nCRSDepTime\n6464\n0\n1\n\n\nDepTime\n0\n0\n160\n\n\nCRSArrTime\n6464\n0\n37\n\n\nArrTime\n0\n0\n710\n\n\n\nThe value counts show some interesting patterns. Both CRS variables CRSDepTime and CRSArrTime have the exact same number of 0s and do not contain missing values. This makes it highly likely that these values represent missing values. Additionally, the 2400 values occur mostly in true time variables and tend more to be arrival times. They are likely to represent midnight, but are not supported by the datetime object.\nGiven the spike of 0 values shown in the histogram above, along with the complete absence of NaN values in the CRS variables, we can conclude that the 0s in the CRS variables represent missing values. We will deal with how to fill these values in a later section.\n\n\nFeature Engineering: Datetime Conversion\nAs we stated above, the HHMM format is not a suitable format for time operations, and is instead must be replaced by a datetime object. We will start by converting the time variables to a datetime object. This will allow us to perform date operations and visualizations.\nHowever, in order to do this, we must also account for the 0s and 2400s in the CRS variables, as well as the NaN values in the true time variables. We will create a function that will convert the time variables to a datetime object, while also accounting for these values.\ndef convert_to_datetime(df, time_cols, date):\n    matrix = df[time_cols].values\n    matrix = matrix.astype('int').astype('str')\n    matrix = np.char.zfill(matrix, 4)\n    matrix[matrix == '0000'] = np.nan\n    matrix[matrix == '2400'] = '0000'\n    df[time_cols] = matrix\n    for col in time_cols:\n        df[col] = pd.to_datetime(date + ' ' + df[col], format=\"%Y-%m-%d %H%M\", errors='coerce')\n    return df\n\ndf = convert_to_datetime(df, time_cols, df['FlightDate'].astype(str))\n\ndf[time_cols]\n\n\n\n\n\n\nCRSDepTime\n\n\nDepTime\n\n\nCRSArrTime\n\n\nArrTime\n\n\n\n\n\n\n0\n\n\n1998-01-02 16:40:00\n\n\n1998-01-02 16:59:00\n\n\n1998-01-02 18:36:00\n\n\n1998-01-02 18:59:00\n\n\n\n\n1\n\n\n2009-05-28 12:04:00\n\n\n2009-05-28 12:02:00\n\n\n2009-05-28 15:41:00\n\n\n2009-05-28 15:41:00\n\n\n\n\n2\n\n\n2013-06-29 16:30:00\n\n\n2013-06-29 16:44:00\n\n\n2013-06-29 19:45:00\n\n\n2013-06-29 19:42:00\n\n\n\n\n3\n\n\n2010-08-31 13:05:00\n\n\n2010-08-31 13:05:00\n\n\n2010-08-31 20:35:00\n\n\n2010-08-31 20:15:00\n\n\n\n\n4\n\n\n2006-01-15 18:20:00\n\n\n2006-01-15 19:11:00\n\n\n2006-01-15 20:26:00\n\n\n2006-01-15 20:58:00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1958943\n\n\n2008-03-23 14:40:00\n\n\n2008-03-23 14:44:00\n\n\n2008-03-23 15:50:00\n\n\n2008-03-23 15:43:00\n\n\n\n\n1958944\n\n\n1999-01-05 09:45:00\n\n\n1999-01-05 09:45:00\n\n\n1999-01-05 12:42:00\n\n\n1999-01-05 12:34:00\n\n\n\n\n1958945\n\n\n2003-11-14 12:25:00\n\n\n2003-11-14 12:19:00\n\n\n2003-11-14 13:19:00\n\n\n2003-11-14 13:08:00\n\n\n\n\n1958946\n\n\n2012-05-15 18:30:00\n\n\n2012-05-15 18:38:00\n\n\n2012-05-15 19:50:00\n\n\n2012-05-15 19:47:00\n\n\n\n\n1958947\n\n\n2003-04-29 16:15:00\n\n\n2003-04-29 16:10:00\n\n\n2003-04-29 17:24:00\n\n\n2003-04-29 17:10:00\n\n\n\n\n\n1958948 rows × 4 columns"
  },
  {
    "objectID": "projects/airline.html#departure-and-arrival-variables-updated",
    "href": "projects/airline.html#departure-and-arrival-variables-updated",
    "title": "Airline On-Time EDA",
    "section": "Departure and Arrival Variables (updated)",
    "text": "Departure and Arrival Variables (updated)\nSo far, we have explore missing value correlations among departure and arrival variables, dropped unnecessary variables, converted the time variables to datetime format, filled missing CRS values, and created binary delay variables. As mentioned at the start of this section, considering the need for data cleaning and the varying nature of missing values, we can now analyze the distribution of the departure and arrival variables in the dataset.\nWe will divide our analysis into two parts: the time variables and the delay variables. Given our data has been cleaned, we can finally focus on the statistical analysis of these variables.\n\nTime Variables\nSince our time data is now clean and in a valid format, we will start by analyzing the distribution of the time variables in the dataset. We will use the time_hist function to create a histogram for the time variables.\nfor col, color in zip(time_cols, colors):\n    times = df[col].apply(lambda x : x.time())\n    times = [t.hour * 60 + t.minute for t in times]\n    plt.hist(times, alpha=0.5, bins=75, color=color)\n    plt.axvline(np.mean(times), color=color, linestyle='dashed', linewidth=2)\nplt.legend(time_cols)\nplt.title('Distribution of CRS and True Depature and Arrivals')\nplt.xticks(np.arange(0, 1441, 60), [f'{h}:00' for h in range(25)])\nplt.show()\n\nThe histogram now shows a clear picture on the distribution of time variables. At first glance, the plot shows departures are more prevalent during morning hours, while arrivals are more predominant during the evening, which is expected. This is further demonstrated by the 2-hour difference in the depature and arrival means, which shows that departures tend to occur earlier on average than arrivals.\nParticularly noticeable is the progression of flight frequencies across the day. The data shows that early morning flights, particularly at 3am, are the least frequent for both depatures and arrivals. However, there is a significant increase in the number of flights at 5am for departures and 7am for arrivals. There is a noticeable of departure flights at 7am, with over 55000 flights registered across the dataset. Afterwards, overall peak hours, where both arrivals and departures are at their highest, remain relatively consistent from 9pm. The number of departures decreases first at 6pm, whereas the number of arrivals starts decreasing from 9pm. This decrease continues past midnight, reaching its lowest point at around 3am.\nMoreover, the distribution of CRS and true time variable pairs for departures and arrivals, respectively, are almost identical, as shown by the close pairs of means. This already reveals that the size of flight delays is small on average across the dataset.\nGiven this insight, let us now focus on the distribution of delay variables in particular.\n\n\nDelay Variables\nunivariate_preview(df, dep_cols[2:] + arr_cols[2:])\n'Data Preview'\n\n\n\n\n\n\n\n\nDepDelay\n\n\nDepDelayMinutes\n\n\nDepDelayBinary\n\n\nDepDel15\n\n\nDepartureDelayGroups\n\n\nArrDelay\n\n\nArrDelayMinutes\n\n\nArrDelayBinary\n\n\nArrDel15\n\n\nArrivalDelayGroups\n\n\n\n\n\n\n0\n\n\n19\n\n\n19\n\n\n1\n\n\n1\n\n\n1\n\n\n23\n\n\n23\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n1\n\n\n-2\n\n\n0\n\n\n0\n\n\n0\n\n\n-1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n14\n\n\n14\n\n\n0\n\n\n0\n\n\n0\n\n\n-3\n\n\n0\n\n\n0\n\n\n0\n\n\n-1\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n-20\n\n\n0\n\n\n0\n\n\n0\n\n\n-2\n\n\n\n\n4\n\n\n51\n\n\n51\n\n\n1\n\n\n1\n\n\n3\n\n\n32\n\n\n32\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\n\n'Value Counts'\n\n\n\n\n\n\n\n\ncolumns\n\n\ndtypes\n\n\nnunique\n\n\ntop5\n\n\nna%\n\n\n\n\n\n\n5\n\n\nArrDelay\n\n\nint64\n\n\n937\n\n\n[0, -5, -3, -2, -4]\n\n\n0.00%\n\n\n\n\n0\n\n\nDepDelay\n\n\nint64\n\n\n911\n\n\n[0, -2, -1, -3, -4]\n\n\n0.00%\n\n\n\n\n1\n\n\nDepDelayMinutes\n\n\nint64\n\n\n855\n\n\n[0, 1, 2, 3, 5]\n\n\n0.00%\n\n\n\n\n6\n\n\nArrDelayMinutes\n\n\nint64\n\n\n850\n\n\n[0, 2, 1, 3, 5]\n\n\n0.00%\n\n\n\n\n4\n\n\nDepartureDelayGroups\n\n\nint64\n\n\n15\n\n\n[-1, 0, 1, 2, 3]\n\n\n0.00%\n\n\n\n\n9\n\n\nArrivalDelayGroups\n\n\nint64\n\n\n15\n\n\n[-1, 0, -2, 1, 2]\n\n\n0.00%\n\n\n\n\n2\n\n\nDepDelayBinary\n\n\nint64\n\n\n2\n\n\n[0, 1]\n\n\n0.00%\n\n\n\n\n3\n\n\nDepDel15\n\n\nint64\n\n\n2\n\n\n[0, 1]\n\n\n0.00%\n\n\n\n\n7\n\n\nArrDelayBinary\n\n\nint64\n\n\n2\n\n\n[0, 1]\n\n\n0.00%\n\n\n\n\n8\n\n\nArrDel15\n\n\nint64\n\n\n2\n\n\n[0, 1]\n\n\n0.00%\n\n\n\n\n\n'Summary Stats'\n\n\n\n\n\n\n\n\nDepDelay\n\n\nDepDelayMinutes\n\n\nDepDelayBinary\n\n\nDepDel15\n\n\nDepartureDelayGroups\n\n\nArrDelay\n\n\nArrDelayMinutes\n\n\nArrDelayBinary\n\n\nArrDel15\n\n\nArrivalDelayGroups\n\n\n\n\n\n\ncount\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n\n\nmean\n\n\n8.495\n\n\n10.400\n\n\n0.160\n\n\n0.169\n\n\n0.062\n\n\n6.197\n\n\n11.785\n\n\n0.189\n\n\n0.198\n\n\n-0.074\n\n\n\n\nstd\n\n\n32.249\n\n\n31.485\n\n\n0.367\n\n\n0.374\n\n\n1.816\n\n\n34.810\n\n\n31.948\n\n\n0.391\n\n\n0.398\n\n\n1.995\n\n\n\n\nmin\n\n\n-990.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-2.000\n\n\n-706.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-2.000\n\n\n\n\n25%\n\n\n-3.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-1.000\n\n\n-10.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-1.000\n\n\n\n\n50%\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-1.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n-1.000\n\n\n\n\n75%\n\n\n6.000\n\n\n6.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n10.000\n\n\n10.000\n\n\n0.000\n\n\n0.000\n\n\n0.000\n\n\n\n\nmax\n\n\n1,878.000\n\n\n1,878.000\n\n\n1.000\n\n\n1.000\n\n\n12.000\n\n\n1,898.000\n\n\n1,898.000\n\n\n1.000\n\n\n1.000\n\n\n12.000\n\n\n\n\n\nThe value counts above show several insights about the delay variables. DepDelay and ArrDelay have the highest number of unique values, since they are continuous variables that include negative values (representing flights ahead of schedule). Interestingly, their top 5 values counts show that on-time and ahead of scheduled flights (by less than 5 minutes) are the most frequent. The delay minutes variables have a lower value count, since they only include positive values. According to the value counts, delays of less than 5 minutes are the most common for both departures and arrivals. The delay groups variables give a deeper glimpse into the amount of delays, as they count the number of 15-minute intervals for flight delays. The most frequent delay groups range from -15 to 30 minutes. The binary delay variables further confirm that the majority of flights are on time.\nThe summary statistics on the other hand show some concerning facts about the DepDelay and ArrDelay variables in particular. According to the summary statistics, the minimum delay is -990 minutes (or -16.5 hours) for departures and -706 minutes (or -11.8 hours) for arrivals. Conversely, the maximum delay for these, as well as for the delay minutes variables, is 1878 minutes or (31.3 hours) for departures and 1898 minutes (or 31.64 hours) for arrivals. This is highly concerning, as it is unlikely for flights to be delayed by such a large amount of time. This clearly indicates the presence of outliers in the dataset.\nBefore exploring this issue, let us first visualize the distribution of the categorical variables in this group to better understand what is really happening with flight delays in the dataset. We will start by creating histograms for the delay variables.\nvariables = dep_cols[4:] + arr_cols[4:]\nncols = 3\nnvars = len(variables)\nfig, axes = plt.subplots(nrows=int(nvars/ncols), ncols=ncols, figsize=(20, 10))\n\nfor col, ax in zip(dep_cols[4:] + arr_cols[4:], axes.flatten()):\n    if df[col].nunique() &gt; 2:\n        ax.hist(df[col], bins=30, edgecolor='black')\n        ax.set_title(col)\n    else:\n        sns.countplot(x=col, data=df, ax=ax, edgecolor='black')\n        ax.set_xlabel(None)\n        ax.set_title(col)\n    plt.tight_layout()\nplt.show()\n\nThe count plots for categorical delay variables give a better understanding as to why the delay variables have such large outliers. The delay binary variables show that around 80% of flights are on time, with only around 20% exhibiting some kind of delay. Morevoer, the 15-minute delay variables have a similar distribution, indicating that the majority of delayed flights are delayed by 15 minutes or more. This amount is considered as the threshold for a delayed flight by the airline industry.\nThe histograms for 15-minute delay groups offer a more detailed view of the distribution of delays, as shown with normalized counts below. Firstly, the distribution is right skewed, with more than 40% of flights ahead of schedule by less than 15 minutes and around 30% of flights on time. These two groups together represent around 70% of flights in the dataset. This is consistent with the value counts above, which show that the majority of flights exhibit no delays greater than 15 minutes (represented in these histograms by values less than or equal to 0).\nBeyond these values, we can better observe the outliers in the dataset. The distribution has a long, right tail, with an increasingly smaller number of flights experiencing delays greater than 15 minutes. A uniformly low count of flights can be seen for delays greater than three 15-minute intervals (or 45 minutes). These are the flights that are causing the large outliers in the delay variables. Finally, it is worth noting that the ArrivalDelayGroups variable even shows a significant amount of flights (around 13%) arriving ahead of schedule by more than 15 minutes.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\nsns.histplot(df['DepartureDelayGroups'], bins=30, ax=axes[0], stat=\"percent\", edgecolor='black')\nsns.histplot(df['ArrivalDelayGroups'], bins=30, ax=axes[1], stat=\"percent\", edgecolor='black')\n\nplt.tight_layout()\nplt.show()\n\nWe will leave the outlier exploration for these variables, as well as any other, for our next notebook covering the bivariate analysis of this dataset. For now, we will move on to the last sections, to analyze the distribution of the remaining variables in the dataset."
  },
  {
    "objectID": "projects/airline.html#flight-summary-variables",
    "href": "projects/airline.html#flight-summary-variables",
    "title": "Airline On-Time EDA",
    "section": "Flight Summary Variables",
    "text": "Flight Summary Variables\nWe will now analyze the distribution of the flight summary variables, which include FlightDate, UniqueCarrier, FlightNum, TailNum, Distance, DistanceGroup, and DivAirportLandings. We will start by analyzing the distribution of these variables and determine any potential issues that may require further processing.\n\nValue Counts\nunivariate_preview(df, sum_cols)\n'Data Preview'\n\n\n\n\n\n\n\n\nCRSElapsedTime\n\n\nActualElapsedTime\n\n\nAirTime\n\n\nDistance\n\n\nDistanceGroup\n\n\n\n\n\n\n0\n\n\n176.000\n\n\n180.000\n\n\n153.000\n\n\n991.000\n\n\n4\n\n\n\n\n1\n\n\n157.000\n\n\n159.000\n\n\n141.000\n\n\n1,066.000\n\n\n5\n\n\n\n\n2\n\n\n135.000\n\n\n118.000\n\n\n103.000\n\n\n773.000\n\n\n4\n\n\n\n\n3\n\n\n270.000\n\n\n250.000\n\n\n220.000\n\n\n1,979.000\n\n\n8\n\n\n\n\n4\n\n\n126.000\n\n\n107.000\n\n\n80.000\n\n\n529.000\n\n\n3\n\n\n\n\n\n'Value Counts'\n\n\n\n\n\n\n\n\ncolumns\n\n\ndtypes\n\n\nnunique\n\n\ntop5\n\n\nna%\n\n\n\n\n\n\n3\n\n\nDistance\n\n\nfloat64\n\n\n1902\n\n\n[337.0, 370.0, 236.0, 328.0, 224.0]\n\n\n0.00%\n\n\n\n\n2\n\n\nAirTime\n\n\nfloat64\n\n\n666\n\n\n[50.0, 45.0, 55.0, 60.0, 53.0]\n\n\n19.32%\n\n\n\n\n1\n\n\nActualElapsedTime\n\n\nfloat64\n\n\n664\n\n\n[70.0, 65.0, 75.0, 60.0, 80.0]\n\n\n0.00%\n\n\n\n\n0\n\n\nCRSElapsedTime\n\n\nfloat64\n\n\n630\n\n\n[70.0, 65.0, 75.0, 80.0, 60.0]\n\n\n0.00%\n\n\n\n\n4\n\n\nDistanceGroup\n\n\nint64\n\n\n11\n\n\n[2, 3, 1, 4, 5]\n\n\n0.00%\n\n\n\n\n\n'Summary Stats'\n\n\n\n\n\n\n\n\nCRSElapsedTime\n\n\nActualElapsedTime\n\n\nAirTime\n\n\nDistance\n\n\nDistanceGroup\n\n\n\n\n\n\ncount\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n1,572,116.000\n\n\n1,948,509.000\n\n\n1,948,509.000\n\n\n\n\nmean\n\n\n127.445\n\n\n125.167\n\n\n106.121\n\n\n736.464\n\n\n3.421\n\n\n\n\nstd\n\n\n70.524\n\n\n70.390\n\n\n68.602\n\n\n569.531\n\n\n2.246\n\n\n\n\nmin\n\n\n0.000\n\n\n3.000\n\n\n-703.000\n\n\n11.000\n\n\n1.000\n\n\n\n\n25%\n\n\n75.000\n\n\n74.000\n\n\n56.000\n\n\n325.000\n\n\n2.000\n\n\n\n\n50%\n\n\n109.000\n\n\n107.000\n\n\n87.000\n\n\n583.000\n\n\n3.000\n\n\n\n\n75%\n\n\n159.000\n\n\n156.000\n\n\n136.000\n\n\n972.000\n\n\n4.000\n\n\n\n\nmax\n\n\n705.000\n\n\n975.000\n\n\n965.000\n\n\n5,095.000\n\n\n11.000\n\n\n\n\n\nThe value counts shows us that most variables except for DistanceGroup are numerical variables, thus exhibiting a high number of unique values. The values are mostly saved as floats, which, as with the departure and arrival variables in previous sections, might be best to save as integers. Regarding top values, we see that the most frequent values for the Distance column range from the low 200s to the high 300s, AirTime values cluster around 50 minutes, and CRSElapsedTime and ActualElapsedTime at 60 minutes. The DistanceGroup variable is a categorical variable that counts the flight distance in 250-mile intervals. All top values are less than 5, indicating that top flights are less than 1250 miles long.\nThe summary statistics show a more detailed understanding of the numerical distribution of the variables, which we visualize below.\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 10))\n(ax1, ax2, ax3, ax4, ax5, ax6) = axes.flatten()\n\nsns.histplot(df['Distance'], bins=30, ax=ax1, kde=True, edgecolor='black')\nsns.countplot(x='DistanceGroup', data=df, ax=ax2, edgecolor='black', stat='percent')\nsns.histplot(df['CRSElapsedTime'], bins=30, ax=ax3, kde=True, edgecolor='black')\nsns.histplot(df['ActualElapsedTime'], bins=30, ax=ax4, kde=True, edgecolor='black')\nsns.histplot(df['AirTime'], bins=30, ax=ax5, kde=True, edgecolor='black')\n\nplt.tight_layout()\nplt.show()\n\nStarting with the Distance variable, the histogram shows the distribution is right skewed, with the majority of flights being less than 1000 miles long. The normalized count plot for DistanceGroup, which groups flights into distance groups 250 Miles, shows a similar distribution. According to the summary statistics for these variables, the mean distance for flights in the dataset is 735 miles, with a standard deviation of 569 miles, and 75% of flights being less than 969 miles long or within the first 4 distance group. Fewer flights are longer, which may be the case for flights to Hawaii, Alaska, as well as to U.S. territories and possessions and cross-country flights.\nThe summary statistics and histograms for CRSElapsedTime, ActualElapsedTime, and AirTime show a different story. The distribution for these variables is also right skewed, with 75% of flights lasting under 160 minutes or 2 hours and 40 minutes from departure to arrival. However, the ActualElapsedTime and, in particular, AirTime variables exhibit negative values, which is not possible for flight times. Two possible explanations for this is that time zone differences, as well as flights spanning across midnight, might have caused the arrival time to be earlier than the departure time. This will require further exploration to confirm, but, if true, would simple require time zone adjustments and simple matrix operations to fix. Finally, the AirTime variable has a large amount of missing values, which will require further processing.\nWe will explore these issues in the following sections, starting with the question of outlying values in the distance variables. Then we will move on to the issue of negative values in the time variables. But first, we will convert the numerical variables to integers.\n\n\nData Cleaning: Changing Data Types\nWe will convert the numerical variables to integers, as they are all counts or durations, and thus do not require decimal points. To allow while keeping the missing values, we will convert the variables to Int64 type and ignore missing values using the errors='coerce' argument from pd.to_numeric.\ndf[sum_cols[:-1]] = df[sum_cols[:-1]].apply(pd.to_numeric, errors='coerce').astype('Int64')\ndisplay(df[sum_cols].dtypes)\nCRSElapsedTime       Int64\nActualElapsedTime    Int64\nAirTime              Int64\nDistance             Int64\nDistanceGroup        int64\ndtype: object\n\n\nExploring Distance Outliers\nWe will start by exploring flights with outlying distances. To achieve this, we will filter the dataset for flights with distances greater than the 3rd quartile and preview the unique Origin and Dest value combinations for these flights.\nlong_flights = df.query(\"Distance &gt; 969\").sort_values(by='Distance', ascending=False)\nprint(f\"Number of flights greater than 3Q (969): {long_flights.shape[0]} or {long_flights.shape[0]/df.shape[0]:.2%}\")\n\ndisplay(df.groupby(['Origin', 'Dest'])['Distance'].max().reset_index().sort_values(by='Distance', ascending=False).head(10))\nNumber of flights greater than 3Q (969): 488141 or 25.05%\n\n\n\n\n\n\n\n\nOrigin\n\n\nDest\n\n\nDistance\n\n\n\n\n\n\n3415\n\n\nHNL\n\n\nBOS\n\n\n5095\n\n\n\n\n934\n\n\nBOS\n\n\nHNL\n\n\n5095\n\n\n\n\n3426\n\n\nHNL\n\n\nJFK\n\n\n4983\n\n\n\n\n4144\n\n\nJFK\n\n\nHNL\n\n\n4983\n\n\n\n\n2854\n\n\nEWR\n\n\nHNL\n\n\n4963\n\n\n\n\n3421\n\n\nHNL\n\n\nEWR\n\n\n4963\n\n\n\n\n3423\n\n\nHNL\n\n\nIAD\n\n\n4817\n\n\n\n\n3620\n\n\nIAD\n\n\nHNL\n\n\n4817\n\n\n\n\n3416\n\n\nHNL\n\n\nCLT\n\n\n4678\n\n\n\n\n1528\n\n\nCLT\n\n\nHNL\n\n\n4678\n\n\n\n\n\nAs suspected, long distances are mostly flights to or from Hawaii. The aggregation above shows that the top 10 longest flights are all to or from Hawaii, with the longest flight being between Honolulu (HNL) and Boston (BOS). According to the histograms from the previous section, these flights are not only outliers, but are also among the least frequent in the dataset.\nBelow we show which are the top 10 most frequent flights not involving Hawaii.\nlong_flights_not_hawaii = long_flights.query(\"~OriginStateName.str.contains('Hawaii') & ~DestStateName.str.contains('Hawaii')\")\ndisplay(long_flights_not_hawaii.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))\n\n\n\n\n\n\n\n\nOrigin\n\n\nDest\n\n\nDistance\n\n\n\n\n\n\n33\n\n\nANC\n\n\nATL\n\n\n3417\n\n\n\n\n638\n\n\nLAX\n\n\nSJU\n\n\n3386\n\n\n\n\n756\n\n\nPHL\n\n\nANC\n\n\n3379\n\n\n\n\n38\n\n\nANC\n\n\nEWR\n\n\n3370\n\n\n\n\n39\n\n\nANC\n\n\nIAH\n\n\n3266\n\n\n\n\n331\n\n\nCVG\n\n\nANC\n\n\n3110\n\n\n\n\n36\n\n\nANC\n\n\nDFW\n\n\n3043\n\n\n\n\n37\n\n\nANC\n\n\nDTW\n\n\n2986\n\n\n\n\n50\n\n\nANC\n\n\nSTL\n\n\n2936\n\n\n\n\n44\n\n\nANC\n\n\nORD\n\n\n2846\n\n\n\n\n\nAs we can see, the longest flights not involving Hawaii are mostly flights betwen Alaska (ANC) and east cost cities like Atlanta (ATL), Philadelphia (PHL), Newark (EWR) and Chicago (IAH). Interestingly, the second longest flight in this groups does not involve either Hawaii or Alaska, but a U.S. territory. This is a flight between San Juan (SJU) and Los Angeles (LAX).\nLastly, we leave below the top 10 longest flights not involving Hawaii or Alaska, which, as expected, mostly involve transcontinental flights.\nlong_flights_not_hawaii_alaska = long_flights.query(\"~OriginStateName.isin(['Hawaii', 'Alaska']) & ~DestStateName.isin(['Hawaii', 'Alaska'])\")\ndisplay(long_flights_not_hawaii_alaska.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))\n\n\n\n\n\n\n\n\nOrigin\n\n\nDest\n\n\nDistance\n\n\n\n\n\n\n618\n\n\nLAX\n\n\nSJU\n\n\n3386\n\n\n\n\n667\n\n\nMIA\n\n\nSEA\n\n\n2724\n\n\n\n\n481\n\n\nFLL\n\n\nSEA\n\n\n2717\n\n\n\n\n199\n\n\nBOS\n\n\nSFO\n\n\n2704\n\n\n\n\n191\n\n\nBOS\n\n\nOAK\n\n\n2694\n\n\n\n\n200\n\n\nBOS\n\n\nSJC\n\n\n2689\n\n\n\n\n203\n\n\nBOS\n\n\nSMF\n\n\n2636\n\n\n\n\n138\n\n\nBDL\n\n\nSFO\n\n\n2625\n\n\n\n\n184\n\n\nBOS\n\n\nLAX\n\n\n2611\n\n\n\n\n185\n\n\nBOS\n\n\nLGB\n\n\n2602\n\n\n\n\n\n\n\nData Cleaning: Negative Time Values\nIn this section, we will explore the issue of negative time values in the ActualElapsedTime and AirTime variables. We will start by filtering the dataset for flights with negative ActualElapsedTime and AirTime values and preview the departure and arrival times (both CRS and real) for these flights, along with their origin and destination states to account for time differences.\nnegative_times = df.query(\"ActualElapsedTime &lt; 0 | AirTime &lt; 0\")[['OriginState', 'DestState', 'CRSDepTime', 'CRSArrTime', 'DepTime', 'ArrTime', 'ActualElapsedTime', 'AirTime']]\n\nprint(f\"Number of negative values in `ActualElapsedTime`: {negative_times.query('ActualElapsedTime &lt; 0').shape[0]}\")\nprint(f\"Number of negative values in `AirTime`: {negative_times.query('AirTime &lt; 0').shape[0]}\")\n\ndisplay(negative_times.query(\"ActualElapsedTime &lt; 0\"))\ndisplay(negative_times.query(\"AirTime &lt; 0\").sample(5))\nNumber of negative values in `ActualElapsedTime`: 0\nNumber of negative values in `AirTime`: 27\n\n\n\n\n\n\n\n\nOriginState\n\n\nDestState\n\n\nCRSDepTime\n\n\nCRSArrTime\n\n\nDepTime\n\n\nArrTime\n\n\nActualElapsedTime\n\n\nAirTime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginState\n\n\nDestState\n\n\nCRSDepTime\n\n\nCRSArrTime\n\n\nDepTime\n\n\nArrTime\n\n\nActualElapsedTime\n\n\nAirTime\n\n\n\n\n\n\n1123316\n\n\nUT\n\n\nMT\n\n\n2004-05-16 22:40:00-06:00\n\n\n2004-05-17 00:05:00-06:00\n\n\n2004-05-16 22:35:00-06:00\n\n\n2004-05-16 23:50:00-06:00\n\n\n75\n\n\n-60\n\n\n\n\n1179139\n\n\nKY\n\n\nOK\n\n\n2004-12-19 09:45:00-05:00\n\n\n2004-12-19 10:57:00-06:00\n\n\n2004-12-19 10:15:00-05:00\n\n\n2004-12-19 11:34:00-06:00\n\n\n139\n\n\n-594\n\n\n\n\n1153657\n\n\nGA\n\n\nFL\n\n\n2004-05-22 09:50:00-04:00\n\n\n2004-05-22 11:18:00-05:00\n\n\n2004-05-22 11:05:00-04:00\n\n\n2004-05-22 12:18:00-05:00\n\n\n73\n\n\n-685\n\n\n\n\n1439783\n\n\nTN\n\n\nKY\n\n\n2004-02-23 10:10:00-06:00\n\n\n2004-02-23 11:15:00-05:00\n\n\n2004-02-23 10:10:00-06:00\n\n\n2004-02-23 11:11:00-05:00\n\n\n61\n\n\n-626\n\n\n\n\n742574\n\n\nNY\n\n\nKY\n\n\n2004-11-10 16:59:00-05:00\n\n\n2004-11-10 19:00:00-05:00\n\n\n2004-11-10 17:10:00-05:00\n\n\n2004-11-10 19:03:00-05:00\n\n\n113\n\n\n-63\n\n\n\n\n\nThe filtered data shows that the number of negative values in these columns is extremely small (&lt;0.01% of the dataset). It is not clear from the sample data how are the values for AirTime calculated, since even with the deleted taxi variables the AirTime values are still not consistent with the difference between WheelsOff and WheelsOn. We decide therefore to drop the AirTime variable, as it is not clear how it is calculated and it is not consistent with the rest of the dataset.\nThe sample data for ActualElapsedTime confirms our hypothesis that the negative values are due to flights spanning across midnight and time zone differences. This leads to an even bigger realization: given the time variables do not contain time zone information, but are set in local times, both the CRSElapsedTime and ActualElapsedTime values do not represent the actual elapsed time for the flights. This is a major issue, since accurate time measurements for flight duraction might be very useful variable for feature engineeering and for our model. We will therefore also drop these features and replace them with an accurate calculation of the elapsed times, accounting for time zone differences.\nprint(f\"Columns before drops: {df.shape[1]}\")\ndf.drop(columns='AirTime', inplace=True)\ndf.drop(columns='CRSElapsedTime', inplace=True)\ndf.drop(columns='ActualElapsedTime', inplace=True)\nsum_cols.remove('AirTime')\nsum_cols.remove('CRSElapsedTime')\nsum_cols.remove('ActualElapsedTime')\nprint(f\"Columns after drops: {df.shape[1]}\")\nColumns before drops: 45\nColumns after drops: 42\n\n\nFeature Engineering: Elapsed Time Calculation\nWith time zones imputed for all time variables, we can accurately calculate the actual elapsed time for flights by converting the time variables to UTC and calculating the time difference between the ArrTime_UTC and DepTime_UTC variables. We will first convert the time variables to UTC.\nfor time_col in time_cols:\n    df[time_col + '_UTC'] = pd.to_datetime(df[time_col], utc=True)\n\ndisplay(df[time_cols + ['OriginState', 'DestState', 'CRSDepTime_UTC', 'DepTime_UTC', 'CRSArrTime_UTC', 'ArrTime_UTC']].head())\n\n\n\n\n\n\n\n\nCRSDepTime\n\n\nDepTime\n\n\nCRSArrTime\n\n\nArrTime\n\n\nOriginState\n\n\nDestState\n\n\nCRSDepTime_UTC\n\n\nDepTime_UTC\n\n\nCRSArrTime_UTC\n\n\nArrTime_UTC\n\n\n\n\n\n\n0\n\n\n1998-01-02 16:40:00-06:00\n\n\n1998-01-02 16:59:00-06:00\n\n\n1998-01-02 18:36:00-07:00\n\n\n1998-01-02 18:59:00-07:00\n\n\nMN\n\n\nUT\n\n\n1998-01-02 22:40:00+00:00\n\n\n1998-01-02 22:59:00+00:00\n\n\n1998-01-03 01:36:00+00:00\n\n\n1998-01-03 01:59:00+00:00\n\n\n\n\n1\n\n\n2009-05-28 12:04:00-05:00\n\n\n2009-05-28 12:02:00-05:00\n\n\n2009-05-28 15:41:00-05:00\n\n\n2009-05-28 15:41:00-05:00\n\n\nWI\n\n\nFL\n\n\n2009-05-28 17:04:00+00:00\n\n\n2009-05-28 17:02:00+00:00\n\n\n2009-05-28 20:41:00+00:00\n\n\n2009-05-28 20:41:00+00:00\n\n\n\n\n2\n\n\n2013-06-29 16:30:00-06:00\n\n\n2013-06-29 16:44:00-06:00\n\n\n2013-06-29 19:45:00-06:00\n\n\n2013-06-29 19:42:00-06:00\n\n\nCO\n\n\nTX\n\n\n2013-06-29 22:30:00+00:00\n\n\n2013-06-29 22:44:00+00:00\n\n\n2013-06-30 01:45:00+00:00\n\n\n2013-06-30 01:42:00+00:00\n\n\n\n\n3\n\n\n2010-08-31 13:05:00-07:00\n\n\n2010-08-31 13:05:00-07:00\n\n\n2010-08-31 20:35:00-05:00\n\n\n2010-08-31 20:15:00-05:00\n\n\nCA\n\n\nMI\n\n\n2010-08-31 20:05:00+00:00\n\n\n2010-08-31 20:05:00+00:00\n\n\n2010-09-01 01:35:00+00:00\n\n\n2010-09-01 01:15:00+00:00\n\n\n\n\n4\n\n\n2006-01-15 18:20:00-05:00\n\n\n2006-01-14 19:11:00-05:00\n\n\n2006-01-15 20:26:00-05:00\n\n\n2006-01-15 20:58:00-05:00\n\n\nNJ\n\n\nNC\n\n\n2006-01-15 23:20:00+00:00\n\n\n2006-01-15 00:11:00+00:00\n\n\n2006-01-16 01:26:00+00:00\n\n\n2006-01-16 01:58:00+00:00\n\n\n\n\n\nNow with the time variables in UTC, we can accurately calculate the actual elapsed time for the flights. We will recreate the CRSElapsedTime and ActualElapsedTime with the difference between the ArrTime_UTC and DepTime_UTC variables.\ndf['CRSElapsedTime'] = ((df['CRSArrTime_UTC'] - df['CRSDepTime_UTC']).dt.total_seconds() / 60).astype('int')\ndf['ActualElapsedTime'] = ((df['ArrTime_UTC'] - df['DepTime_UTC']).dt.total_seconds() / 60).astype('int')\n\nsum_cols.append('CRSElapsedTime')\nsum_cols.append('ActualElapsedTime')\n\ndisplay(df[sum_cols].head())\n\n\n\n\n\n\n\n\nDistance\n\n\nDistanceGroup\n\n\nCRSElapsedTime\n\n\nActualElapsedTime\n\n\n\n\n\n\n0\n\n\n991\n\n\n4\n\n\n176\n\n\n180\n\n\n\n\n1\n\n\n1066\n\n\n5\n\n\n217\n\n\n219\n\n\n\n\n2\n\n\n773\n\n\n4\n\n\n195\n\n\n178\n\n\n\n\n3\n\n\n1979\n\n\n8\n\n\n330\n\n\n310\n\n\n\n\n4\n\n\n529\n\n\n3\n\n\n126\n\n\n1547\n\n\n\n\n\n\n\nValue Counts (updated)\nNow that we have cleaned the flight summary variables and recalculated our elapsed time variables, we can now analyze the distribution of these variables. Below we plot the elapsed time variables.\nfig, axs = plt.subplots(2, 2, gridspec_kw={\"height_ratios\": (.2, .8)}, figsize=figsize)\naxs = axs.ravel()\n\nfor i, col in enumerate(['CRSElapsedTime', 'ActualElapsedTime']):\n    sns.boxplot(x=df[col], ax=axs[i]).set_xlabel('')\n    sns.histplot(df[col], ax=axs[i+2])\n\nx_range = df[['CRSElapsedTime', 'ActualElapsedTime']].values.flatten()\nx_min, x_max = x_range.min(), x_range.max()\nfor ax in axs:\n    ax.set_xlim(x_min - 100, x_max + 100)\n\nplt.tight_layout()\nplt.show()\n\nThe distributions for the CRSElapsedTime and ActualElapsedTime variables are right skewed, with the majority of flights lasting less than"
  },
  {
    "objectID": "projects/airline.html#conclusion",
    "href": "projects/airline.html#conclusion",
    "title": "Airline On-Time EDA",
    "section": "Conclusion",
    "text": "Conclusion\n\nTakeaways\n\n\nExporting Dataset and Variables\ndf.to_parquet('./data/interim/04-airline_2m.parquet', index=False)\nvariables = {name: value for name, value in locals().items() if name.endswith('_cols')}\n\nwith open('./src/modules/variables_04.py', 'w') as f:\n    for name, value in variables.items():\n        f.write(f\"{name} = {value}\\n\")"
  },
  {
    "objectID": "projects/biodiversity.html",
    "href": "projects/biodiversity.html",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "projects/biodiversity.html#introduction",
    "href": "projects/biodiversity.html#introduction",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "projects/biodiversity.html#importing-modules-and-data-from-files",
    "href": "projects/biodiversity.html#importing-modules-and-data-from-files",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Importing Modules and Data from Files",
    "text": "Importing Modules and Data from Files\nFirst, we will import the preliminary modules for this project, along with the data from the two separate files provided for this analysis.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Set default figure size\n# figsize = (15,9)\nfigsize = (10,6)\nplt.rcParams['figure.figsize'] = figsize\nsns.set(rc={'figure.figsize':figsize})\n\n# Set default float size\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nobservations = pd.read_csv('observations.csv')\nspecies = pd.read_csv('species_info.csv')\nImport successful"
  },
  {
    "objectID": "projects/biodiversity.html#preview-the-data",
    "href": "projects/biodiversity.html#preview-the-data",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Preview the Data",
    "text": "Preview the Data\nTo prepare for our exploratory data analysis, we’ll first conduct an initial preview of the data. This will involve sampling a subset of the data and inspecting its structure and characteristics.\n\nspecies.csv\nLet’s begin by examening the species dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(species.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(species.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n718\nVascular Plant\nPogonia ophioglossoides\nPogonia, Rose Pogonia\nNaN\n\n\n1361\nVascular Plant\nLespedeza stuevei\nTall Lespedeza\nNaN\n\n\n4725\nVascular Plant\nCalycadenia mollis\nSoft Western Rosinweed\nNaN\n\n\n2912\nNonvascular Plant\nThuidium allenii\nAllen's Thuidium Moss\nNaN\n\n\n1929\nVascular Plant\nPicea abies\nNorway Spruce\nNaN\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5824 entries, 0 to 5823\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   category             5824 non-null   object\n 1   scientific_name      5824 non-null   object\n 2   common_names         5824 non-null   object\n 3   conservation_status  191 non-null    object\ndtypes: object(4)\nmemory usage: 182.1+ KB\n\nNone\nThe species dataset shows 5824 entries with four variables:\n\ncategory: taxonomy for each species.\nscientific_name: scientific name of each species.\ncommon_names: common names of each species.\nconservation_status: species’ conservation status.\n\nUpon inspection with .info(), we observe that the conservation_status column contains 191 non-null entries, indicating a high presence of missing values. While the majority of columns may retain their data type as objects, an argument could be made for converting conservation_status to an ordinal variable. However, due to the presence of incomplete conservation statuses and the ambiguity surrounding the ordinal nature of in recovery, we’ll retain it as an object.\n\n\nobservations.csv\nWe’ll now move on to the observations dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(observations.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(observations.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\nscientific_name\npark_name\nobservations\n\n\n\n\n21462\nLepomis humilis\nYellowstone National Park\n222\n\n\n1305\nSaxifraga odontoloma\nYosemite National Park\n116\n\n\n1307\nPerdix perdix\nYosemite National Park\n162\n\n\n20947\nFraxinus profunda\nBryce National Park\n129\n\n\n10240\nMuhlenbergia andina\nYellowstone National Park\n235\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n\nRangeIndex: 23296 entries, 0 to 23295\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   scientific_name  23296 non-null  object\n 1   park_name        23296 non-null  object\n 2   observations     23296 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 546.1+ KB\n\nNone\nThe observations dataset consists of three columns:\n\nscientific_name: scientific name of each species.\npark_name: name of the national park species are located in.\nobservations: number of observations in the past 7 days.\n\nBased on the information above, the columns don’t show any missing data, and the data types seem to be appropriate for the analysis."
  },
  {
    "objectID": "projects/biodiversity.html#exploratory-data-analysis",
    "href": "projects/biodiversity.html#exploratory-data-analysis",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nspecies.csv\nLet’s delve deeper into the species dataset to gain insights into its characteristics and identify any anomalies or patterns. We’ll begin by employing a custom function column_eda() to analyze each column:\ndef column_eda(dataset):\n    cols = list(dataset.columns)\n    for col in cols:\n        print(f'---------------{col}---------------')\n        print(f'Unique values:', dataset[col].nunique(), \n              f'Non-null values: {dataset[col].notnull().sum()}',\n              f'Missing values: {dataset[col].isnull().sum()}\\n', \n              sep='\\n')\n        print(dataset[col].value_counts().head(4))\n\ncolumn_eda(species)\n---------------category---------------\nUnique values:\n7\nNon-null values: 5824\nMissing values: 0\n\ncategory\nVascular Plant       4470\nBird                  521\nNonvascular Plant     333\nMammal                214\nName: count, dtype: int64\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 5824\nMissing values: 0\n\nscientific_name\nCastor canadensis       3\nCanis lupus             3\nHypochaeris radicata    3\nColumba livia           3\nName: count, dtype: int64\n---------------common_names---------------\nUnique values:\n5504\nNon-null values: 5824\nMissing values: 0\n\ncommon_names\nBrachythecium Moss    7\nDicranum Moss         7\nPanic Grass           6\nBryum Moss            6\nName: count, dtype: int64\n---------------conservation_status---------------\nUnique values:\n4\nNon-null values: 191\nMissing values: 5633\n\nconservation_status\nSpecies of Concern    161\nEndangered             16\nThreatened             10\nIn Recovery             4\nName: count, dtype: int64\nThe function shows there are 7 categories of species, 5541 species, 5504 common names and 4 conservation statuses. From the analysis, several insights emerge:\n\nMissing Conversation Statuses: the conservation_status column exhibits a high number of nan values (5633), which could be interpreted as ‘species of no concern’ or requiring ‘no intervention’.\n\nTo address this, we’ll impute the missing values with “No intervention”, expanding the conservation status categories to five.\nprint('Old conservation status:\\n', list(species.conservation_status.unique()))\n\nspecies.conservation_status = species.conservation_status.fillna('No intervention')\n\nprint('New conservation status:\\n', list(species.conservation_status.unique()))\nOld conservation status:\n [nan, 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\nNew conservation status:\n ['No intervention', 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\n{:start=“2”} 2. Duplicate Entries: there is a discrepancy between the number of unique values of scientific_name and common_names despite all entries having non-null values. This points to the presence of duplicate common names for different species.\nWe’ll confirm this by identifying and examining these duplicates.\nduplicates = species.duplicated().sum()\nprint(f'Overall duplicates (rows): {duplicates}')\n\nrepeated_scientific_names = species.duplicated(subset=['scientific_name']).sum()\nprint(f'Duplicated scientific names: {repeated_scientific_names}')\n\nrepeated_common_names = species.duplicated(subset=['common_names']).sum()\nprint(f'Duplicated common names: {repeated_common_names}')\nOverall duplicates (rows): 0\nDuplicated scientific names: 283\nDuplicated common names: 320\nTo illustrate, we’ll display the most frequent common name alongside its associated scientific names.\ndisplay(species.common_names.value_counts().reset_index()[:5])\ndisplay(species.query(\"common_names == 'Brachythecium Moss'\")[['common_names', 'scientific_name']])\n\n\n\n\ncommon_names\ncount\n\n\n\n\n0\nBrachythecium Moss\n7\n\n\n1\nDicranum Moss\n7\n\n\n2\nPanic Grass\n6\n\n\n3\nBryum Moss\n6\n\n\n4\nSphagnum\n6\n\n\n\n\n\n\n\ncommon_names\nscientific_name\n\n\n\n\n2812\nBrachythecium Moss\nBrachythecium digastrum\n\n\n2813\nBrachythecium Moss\nBrachythecium oedipodium\n\n\n2814\nBrachythecium Moss\nBrachythecium oxycladon\n\n\n2815\nBrachythecium Moss\nBrachythecium plumosum\n\n\n2816\nBrachythecium Moss\nBrachythecium rivulare\n\n\n2817\nBrachythecium Moss\nBrachythecium rutabulum\n\n\n2818\nBrachythecium Moss\nBrachythecium salebrosum\n\n\n\nAs seen above, the most frequent common name is Brachythecium Moss, with a total of 7 different species identified with this name. Organisms in this example all share the same genus (i.e. brachythecium, a genus of moss), but differ in species, thus the different scientific names.\nThis demonstrates instances where multiple species share identical common names but differ in scientific nomenclature.\n{:start=“3”} 3. Duplicate Scientific Names: the presence of duplicate scientific names suggests repeated observations of the same species, since the dataset should report the conservation status of each species, thus one observation per species.\nSince there are no overall duplicates in the dataset (see above), these duplicate names must have some difference at the row level. To confirm this, we’ll print out a sample of duplicates and inspect three random duplicates species, to see what kind of differences are there within the rows themselves.\nduplicated_species = species[species['scientific_name'].duplicated(keep=False)]\n\ndisplay('-------Sample of duplicated scientific names-------')\ndisplay(duplicated_species.head())\n\ndef display_duplicated_species(scientific_name):\n    duplicated_entries = duplicated_species[duplicated_species['scientific_name'] == scientific_name]\n    display(f'-------Duplicated \\'{scientific_name}\\'-------')\n    display(duplicated_entries)\n\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\nfor scientific_name in scientific_names_to_check:\n    display_duplicated_species(scientific_name)\n'-------Sample of duplicated scientific names-------'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n6\nMammal\nSus scrofa\nFeral Hog, Wild Pig\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n10\nMammal\nUrocyon cinereoargenteus\nCommon Gray Fox, Gray Fox\nNo intervention\n\n\n\n\"-------Duplicated 'Cervus elaphus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n3017\nMammal\nCervus elaphus\nRocky Mountain Elk\nNo intervention\n\n\n\n\"-------Duplicated 'Canis lupus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n3020\nMammal\nCanis lupus\nGray Wolf, Wolf\nIn Recovery\n\n\n4448\nMammal\nCanis lupus\nGray Wolf, Wolf\nEndangered\n\n\n\n\"-------Duplicated 'Odocoileus virginianus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n3019\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer, White-Tailed Deer\nNo intervention\n\n\n\nIt seems that both the number of common names and the types of conservation statuses are different for duplicate observations. That is, the same species exhibits both different common names, as well as conservation statuses. To solve the question of duplicates, given the differences in conversation statuses do not affect our question on the likelihood of endangerment given a species’ protection status, I’ll retain the first instance of these duplicates.\nspecies = species.drop_duplicates(subset=['scientific_name'], keep='first')\n\nrepeated_scientific_names = species.scientific_name[species.scientific_name.duplicated()]\nprint(f'Duplicated scientific names: {len(repeated_scientific_names)}\\n')\n\nprint('-------Previously duplicated examples (now clean)-------')\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\ndisplay(species[species['scientific_name'].isin(scientific_names_to_check)])\nDuplicated scientific names: 0\n\n-------Previously duplicated examples (now clean)-------\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n\n\n\nobservations.csv\nLet’s extend our exploratory analysis to the observations dataset, mirroring the approach applied to the species dataset. We’ll begin by employing the column_eda() function to analyze each column.\ncolumn_eda(observations)\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 23296\nMissing values: 0\n\nscientific_name\nMyotis lucifugus        12\nPuma concolor           12\nHypochaeris radicata    12\nHolcus lanatus          12\nName: count, dtype: int64\n---------------park_name---------------\nUnique values:\n4\nNon-null values: 23296\nMissing values: 0\n\npark_name\nGreat Smoky Mountains National Park    5824\nYosemite National Park                 5824\nBryce National Park                    5824\nYellowstone National Park              5824\nName: count, dtype: int64\n---------------observations---------------\nUnique values:\n304\nNon-null values: 23296\nMissing values: 0\n\nobservations\n84    220\n85    210\n91    206\n92    203\nName: count, dtype: int64\nThe column analysis revelas the following insights. There are 23296 observations of 5541 unique species documented in 4 parks. The number of species (scientific_name) in the observations datset coincides with the number of species in the species dataset. This suggest that the observations dataset contains observations of all species in the species dataset. To confirm this, we’ll check if the scientific_name column in the observations dataset is a subset of the scientific_name column in the species dataset.\nspecies_names = species.scientific_name\nobservations_names = observations.scientific_name\n\nprint(f'Is the observations dataset a subset of the species dataset? {observations_names.isin(species_names).all()}')\nIs the observations dataset a subset of the species dataset? True\nThe result confirms that the observations dataset is a subset of the species dataset, as all species in the observations dataset are also present in the species dataset.\nFurthermore, as observations is a numerical variable, its distribution provides insights into the frequency of species sightings. To better explore this column given its data type, we’ll visualize the distribution using a histogram.\nsns.histplot(x='observations', data=observations, kde=True)\nplt.show()\n\nThe distribution of in the number of observations seems to follow a multimodal distribution, with at least three discernible peaks in the data: one at 80, another at 150, and a third at 250. This may suggest that the overall distribution is a combination of several distributions, grouped by a certain variable. Given the low number of disceernible peaks, this variable might be the park_name variable. That is: the distribution in the number of observations may be influenced by the size of the parks they were made in.\nTo confirm this, we’ll plot the distribution of observations per park using the hue parameter in the seaborn histplot function.\nsns.histplot(x='observations', data=observations, kde=True, hue='park_name')\nplt.show()\n\nAs suspected, the distribution of observations is indeed influenced by the park in which they were made. The peaks in the distribution clearly correspond to each of the four parks in the dataset. This proves that the number of observations is influenced by the park in which they were made.\n\n\nSummary\nTo encapsulate the insights obtained from our Exploratory Data Analysis (EDA), we present the key characteristics of both datasets.\n\nspecies\n\nDataset Overview: the data comprises 5,824 entries with 4 variables—category, scientific_name, common_names, and conservation_status—offering a diverse array of taxonomic information.\nMissing Values: the conservation_status column contains 5,633 missing values, which were imputed with “No intervention” to account for species not under any conservation status.\nDuplicates: the dataset contains no overall duplicates, but does exhibit duplicate scientific names, which were resolved by retaining the first instance of each duplicate.\nCommon Names: the dataset contains 5541 species, with some sharing identical common names but differing in scientific nomenclature.\nConservation Status: the dataset reports 5 conservation statuses, with most species not under any conservation status.\n\n\n\nobservations\n\nDataset Overview: the data consists of 23,296 entries with 3 variables—scientific_name, park_name, and observations—documenting species sightings in 4 national parks over 7 days.\nUnique Species: the dataset contains observations of 5,541 unique species, all of which are present in the species dataset.\nMissing Values: the dataset contains no missing values, with all columns having non-null entries.\nDistribution: the number of observations followed a multimodal distribution, which was influenced by the park in which observations were conducted."
  },
  {
    "objectID": "projects/biodiversity.html#analysis-1",
    "href": "projects/biodiversity.html#analysis-1",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Analysis",
    "text": "Analysis\nIn this section, we aim to address the questions posed earlier by analyzing the species dataset and later exploring the observations dataset.\n\nQ: What is the distribution of conservation status for animals?\nTo gain insights into the distribution of conservation statuses among animal categories, we begin by aggregating the conservations statuses per species category and calculating both discrete and normalized counts. We then visualize the normalized counts using a stacked bar chart.\ncategory_conservation = pd.crosstab(species['conservation_status'], species['category']).drop(index='No intervention')\ndisplay(category_conservation)\n\ncategory_conservation_norm = pd.crosstab(species['conservation_status'], species['category'], normalize='index').drop(index='No intervention')\ndisplay(category_conservation_norm.style.background_gradient(cmap='Blues', axis=1, vmin=0, vmax=1))\n\nax = category_conservation_norm.plot(kind='bar', stacked=True)\nax.set_xlabel('Conservation Status')\nax.set_ylabel('Number of Species')\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title(\"Distribution of Species Among Conservation Statuses\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n\n\n\n\n\n\n\n\n\nEndangered\n1\n4\n3\n6\n0\n0\n1\n\n\nIn Recovery\n0\n3\n0\n0\n0\n0\n0\n\n\nSpecies of Concern\n4\n68\n4\n22\n5\n5\n43\n\n\nThreatened\n2\n0\n3\n2\n0\n0\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n \n \n \n \n \n \n \n\n\nEndangered\n0.066667\n0.266667\n0.200000\n0.400000\n0.000000\n0.000000\n0.066667\n\n\nIn Recovery\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nSpecies of Concern\n0.026490\n0.450331\n0.026490\n0.145695\n0.033113\n0.033113\n0.284768\n\n\nThreatened\n0.222222\n0.000000\n0.333333\n0.222222\n0.000000\n0.000000\n0.222222\n\n\n\n\nThe table and stacked bar chart above reveal several insights about the distribution of conservation status among different categories of species.\nFirstly, the only animal in recovery are birds, of which there are 3 species making up 100% of this status. This points to the fact that birds are the only species in recovery at the time of the dataset. Moreover, mammals, birds and fish are the most endangered species in the dataset, making more than 85% of all endangered species. Furthermore, more than 70% of species of concern consist of birds and vascular plants. Lastly, the threatened status is almost equally distributed among all species categories, except birds, nonvascular plants and reptiles.\nOverall, the distribution of animals among conservations statuses support the following conclusions:\n\nThe most endangered animals in the dataset consist of mammals, birds and fishes.\nBirds are the only species in recovery, with only 3 species documented.\nThe most common conservation status is species of concern, with birds and vascular plants making up the majority of this category.\nThe threatened status is almost equally distributed among amphibians, fish, mammals and vascular plants.\n\n\n\nQ: Are certain types of species more likely to be endangered?\nThe next question concerns the relation between species and their conservation status. To answer this question requires establishing a definition of likelihood for endangerment. Given protection measures are not documented in the dataset, we can only establish a definition based on the available variables. Therefore, we consider species to be more likely to be engangered if they are classified as endangered, threatened, or species of concern and if no protection measures are placed in response to their endangerment.\nTo answer this question, we create a new protected column with True for all conservations statuses that are not No intervention nor In recovery. We then calculate the relative frequencies of protected and protected species per category. We visualize the results then using a bar chart.\nspecies['protected'] = species.conservation_status.isin(['No intervention', 'In Recovery'])\n\ncategory_protections = pd.crosstab(species['category'], species['protected'], normalize='index')\ndisplay(category_protections)\n\nax = sns.barplot(data = category_protections, y = category_protections.iloc[:, 0]*100, x = 'category')\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.title('Percentage of Likely Endangerement per Species Category')\nplt.ylabel('Percentage Not Protected')\nplt.xlabel('Category')\nplt.show()\n\n\n\nprotected\nFalse\nTrue\n\n\n\n\ncategory\n\n\n\n\nAmphibian\n0.09\n0.91\n\n\nBird\n0.15\n0.85\n\n\nFish\n0.08\n0.92\n\n\nMammal\n0.17\n0.83\n\n\nNonvascular Plant\n0.02\n0.98\n\n\nReptile\n0.06\n0.94\n\n\nVascular Plant\n0.01\n0.99\n\n\n\n\nBased on the information from the bar chart, we can see that mammals and birds have the highest percentage of no protection, with roughly 17% and 15% of species exhibiting some level of engangered, respectively. This suggests that mammals and birds are the most likely to be endangered among the categories.\n\n\nQ: Are the differences between species and their conservation status significant?\nThe question of statistical significance for categorical variables is answered in statistics by use of the chi-square test.\nCrosstabulating both variables would yield a complex result, thus it’s better to break down the question into pairs of species categories. Since based on the previous question mammals are the most likely category to be endangered, we’ll compare the significance of other category differences with mammals.\nWe’ll start by permutating the pairs of categories with mammals. Then I’ll loop over this list to perform the chi-square tests for each pair and plot the p-values to find the statistically significant differences among category pairs.\ncategories = list(species.category.unique())\ncombinations_mammal = [['Mammal', i] for i in categories][1:]\n\ncategory_protections_counts = pd.crosstab(species['category'], species['protected'])\n\nsignificance_data = {'Animal Pair': [], 'p-value': []}\nfor pair in combinations_mammal:\n  contingency_table = category_protections_counts.loc[pair]\n  chi2, pval, dof, expected = chi2_contingency(contingency_table)\n\n  significance_data['Animal Pair'].append(f'{pair[0]} vs {pair[1]}')\n  significance_data['p-value'].append(pval)\n\nsign_data = pd.DataFrame(significance_data)\nsign_data['p-value'] = sign_data['p-value']*100\n# display(sign_data)\n\n# Plot\nplt.subplots(figsize=(10,5))\nax =sns.barplot(data = sign_data, x = 'Animal Pair', y = 'p-value')\nplt.title('Statistical Significance of Protection Statuses per Animal\\n(difference with mammals)')\nplt.axhline(5, color='red', linestyle='--')\nax.set_xlabel(\"\")\nax.set_ylabel('p-value\\n(alpha = 5%)')\nplt.xticks(rotation=45)\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.show()\n\nThe above graph illustrates the p-values for the chi-square tests performed for each animal category against mammals. Given an alpha of 5%, the analysis shows that birds and amphibians display no statistically significant differences in their conservations statuses compared with mammals. However, all other categories such as reptiles, fishes and plants show statistically significant differences in their conservation statuses when comapred to mammals. This means that the conservation statuses of these categories are significantly different from mammals.\n\n\nQ: Which species were spotted the most at each park?\nLastly, we explore the observations dataset to identify the most frequently spotted species in each park.\nSince the dataset doesn’t include common names, we’ll map the common names from the species dataset to the scientific names in the observations dataset. Then, we’ll aggregate the data by park and by species, summing their observations to identify the most frequently spotted species in each park.\nmerged_df = observations.merge(species[['category', 'scientific_name', 'common_names']], how='left').drop_duplicates()\nmerged_df_grouped = merged_df.groupby(['park_name', 'scientific_name', 'common_names']).observations.sum().reset_index()\nmerged_df_grouped = merged_df_grouped.loc[merged_df_grouped.groupby('park_name')['observations'].idxmax()].sort_values(by = 'observations', ascending=False)\n\ndisplay(merged_df_grouped.head())\n\n\n\n\n\n\n\n\n\n\n\npark_name\nscientific_name\ncommon_names\nobservations\n\n\n\n\n13534\nYellowstone National Park\nHolcus lanatus\nCommon Velvet Grass, Velvetgrass\n805\n\n\n19178\nYosemite National Park\nHypochaeris radicata\nCat's Ear, Spotted Cat's-Ear\n505\n\n\n1359\nBryce National Park\nColumba livia\nRock Dove\n339\n\n\n10534\nGreat Smoky Mountains National Park\nStreptopelia decaocto\nEurasian Collared-Dove\n256\n\n\n\nBased on the aggregation above, in Yellowstone National Park, the species Holcus lanatus was the most commonly observed, with a total of 805 sightings. Meanwhile, Hypochaeris radicata was the predominant species in Yosemite National Park, with 505 observations. In Bryce National Park, Columba livia garnered the highest number of sightings, totaling 339. Finally, in Great Smoky Mountains National Park, Streptopelia decaocto was the most frequently spotted species, with 256 observations."
  },
  {
    "objectID": "projects/biodiversity.html#conclusions",
    "href": "projects/biodiversity.html#conclusions",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Conclusions",
    "text": "Conclusions\nThis project set out to explore biodiversity data from the National Parks Service, focusing on endangered species and their conservation statuses. Through a detailed exploratory data analysis, several key findings emerged, shedding light on the distribution of conservation statuses among different species categories, the likelihood of species endangerment, the significance of differences in conservation statuses among species categories, and the most frequently spotted species in each national park.\n\nDistribution of Conservation Statuses\nThe analysis revealed that mammals, birds, and fishes are the most endangered species categories, making up the majority of the endangered conservation status. Birds were the only category with species classified as in recovery, indicating a unique conservation status among all the categories. Out of 178 species marked with some conservation status other than no intervention, most species are under the status of species of concern, especially birds and vascular plants.\n\n\nLikelihood of Species Endangerment\nMammals and birds emerged as the most likely categories to be endangered, with approximately 17% and 15% of species not classified as either in recovery or no intervention. Without any protection measures, this suggests a higher vulnerability to endangerment among mammals and birds compared to other species categories.\n\n\nSignificance of Conservation Status Differences\nStatistical significance testing showed that birds and amphibians did not exhibit statistically significant differences in their conservation statuses compared with mammals. However, all other categories, including reptiles, fishes, and plants, displayed significant differences in conservation statuses compared with mammals. This highlights the importance of considering species-specific conservation measures based on their unique characteristics.\n\n\nMost Frequently Spotted Species\nThe analysis identified the most frequently spotted species in each national park. Species such as common velvet grass, a vascular plant, in Yellowstome National Park. Moreover, doves were the most commonly observed species both in Bryce and Great Smoky Mountains National Parks. Furthermore, the most observed species Yosemite National Park was the cat’s ear plant. This findings are examples of the rich biodiversity present in national parks.\nIn conclusion, this project contributes to our understanding of endangered species and their conservation statuses, highlighting the need for targeted conservation efforts to protect vulnerable species and preserve biodiversity in national parks. Further research could explore additional factors influencing species endangerment and conservation strategies tailored to specific species categories. By understanding and honoring the unique needs of each species category, we can forge a path towards sustainable coexistence and ensure the enduring legacy of our national parks for generations to come."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nAbout this site\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  }
]