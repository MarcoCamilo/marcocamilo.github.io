---
title: BERT, Encoders and Linear Models for Resume Text Classification
subtitle: Exploring Performance of Advanced NLP Algorithms for Text Classification
image: /assets/projects/resume-analyzer/resume-analyzer.jpg
description: Explores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.
featured: 1
category: NLP
type: Deep Learning
skills:
    - PyTorch
    - Tensorflow
    - HuggingFace
    - Scikit-Learn
    - SBERT
    - Gensim
    - NLTK
    - PyLDAVis
---

![]({{< meta image >}})

## Executive Summary

The project explores the performance of advanced NLP models and vectorization techniques specifically for text classification using a dataset of resumes from various job categories. The algorithms tested include Linear SVC, Feedforward Neural Networks (FNN), Encoder models, and BERT, implemented using Scikit-Learn and PyTorch. The project aims to compare the performance of these algorithms on text classification and evaluate their effectiveness in analyzing resume data.

> ### Key Findings
> - **Key Factors in Model Performance**: quality of the feature representations and the ability to capture contextual information across dependencies.
> - **Model performance**
>   - **BERT**: Best performing model with an accuracy of **91.67%**. Showcases the effectiveness of pre-trained models and transfer learning.
>   - **Linear SVC**: Achieved an accuracy of **87.15%** with TF-IDF vectors. Attributed to the model's simplicity and use of effective feature representation.
>   - **Encoder Model**: Achieved an accuracy of **74.54%**. Suggests the need for further fine-tunning, give the difference with its cousin transformer model.
>   - **Feedforward Neural Network**: achieved an accuracy of **73.15%**, the lowest of the four models. Indicates struggle with sequential dependencies and contextual nuances.
> - **Preprocessing and Vectorization**: effective text preprocessing and robust vectorization techniques significantly enhanced overall model performance (> 70%).

## Dataset

The project primarily makes use of the [Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) sourced from LiveCareer, available at Kaggle. The dataset consists of a collection of more than 2400 resumes in both string and HTML format, along with their respective labeled categories. The dataset consists of the following variables:

::: {.column-margin}
![](/assets/projects/resume-analyzer/resume-dataset.png)
:::

- **`ID`**: Unique identifier for each resume
- **`Resume_str`**: Text content of the resume
- **`Resume_html`**: HTML content of the resume
- **`Category`**: Categorized field of the resume (e.g. Information-Technology, Teacher, Advocate, Business-Development, Healthcare)

## Preprocessing

Data preprocessing primarily involved text preprocessing, as well as data rebalancing for the categories in the resume dataset. 

For text cleaning, I used a custom `preprocessing` function that streamlines multiple text cleaning and preprocessing operations. The function is highly flexible, offering tunable parameters to adapt the preprocessing pipeline to include key operations such as lowercase conversion, HTML decoding, email and URL removal, special character removal, contraction expansion, custom regex cleaning, as well as tokenization, stemming, lemmatization, and stopwords removal. 

In particular, I enhanced the text preprocessing by removing noise from the text data, such as non-existen words and frequent, document-specific stopwords like months, section headings and other common words in resumes.

```{python}
import re
from bs4 import BeautifulSoup
from unidecode import unidecode
import contractions
from nltk.corpus import stopwords, words
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

def preprocessing(
    text, 
    tokenize=False,
    stem=False,
    lem=False,
    html=False,
    exist=False,
    remove_emails=True,
    remove_urls=True,
    remove_digits=True,
    remove_punct=True,
    expand_contractions=True,
    remove_special_chars=True,
    remove_stopwords=True,
    lst_stopwords=None,
    lst_regex=None
) -> str | list[str]:
    
    # Lowercase conversion
    cleaned_text = text.lower()

    # HTML decoding
    if html:
        soup = BeautifulSoup(cleaned_text, "html.parser")
        cleaned_text = soup.get_text()
    
    # Remove Emails
    if remove_emails:
        cleaned_text = re.sub(r"([a-z0-9+._-]+@[a-z0-9+._-]+\.[a-z0-9+_-]+)", " ", cleaned_text)
    
    # URL removal
    if remove_urls:
        cleaned_text = re.sub(r"(http|https|ftp|ssh)://[\w_-]+(?:\.[\w_-]+)+[\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-]?", " ", cleaned_text)
    
    # Remove escape sequences and special characters
    if remove_special_chars:
        cleaned_text = re.sub(r"[^\x00-\x7f]", " ", cleaned_text)
        cleaned_text = unidecode(cleaned_text)
    
    # Remove multiple characters
    cleaned_text = re.sub(r"(.)\1{3,}", r"\1", cleaned_text)
    
    # Expand contractions
    if expand_contractions:
        cleaned_text = contractions.fix(cleaned_text)
        cleaned_text = re.sub("'(?=[Ss])", "", cleaned_text)
    
    # Remove digits
    if remove_digits:
        cleaned_text = re.sub(r"\d", " ", cleaned_text)
    
    # Punctuation removal
    if remove_punct:
        cleaned_text = re.sub("[!\"#$%&\\'()*+\,-./:;<=>?@\[\]\^_`{|}~]", " ", cleaned_text)
    
    # Line break and tab removal
    cleaned_text = re.sub(r"[\n\t]", " ", cleaned_text)
    
    # Excessive spacing removal
    cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
    
    # Regex (in case, before cleaning)
    if lst_regex: 
        for regex in lst_regex:
            compiled_regex = re.compile(regex)
            cleaned_text = re.sub(compiled_regex, '', cleaned_text)

    # Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)
    if stem or lem or remove_stopwords or tokenize:
        if isinstance(cleaned_text, str):
            cleaned_text = cleaned_text.split()
        
        # Remove stopwords
        if remove_stopwords:
            if lst_stopwords is None:
                lst_stopwords = set(stopwords.words('english'))
            cleaned_text = [word for word in cleaned_text if word not in lst_stopwords]

        # Remove non-existent words
        if exist:
            english_words = set(words.words())
            cleaned_text = [word for word in cleaned_text if word in english_words]

        # Stemming
        if stem:
            stemmer = PorterStemmer()
            cleaned_text = [stemmer.stem(word) for word in cleaned_text]

        # Lemmatization
        if lem:
            lemmatizer = WordNetLemmatizer()
            cleaned_text = [lemmatizer.lemmatize(word) for word in cleaned_text]
        
        if not tokenize:
            cleaned_text = ' '.join(cleaned_text)

    return cleaned_text
```

Next, I prepared the `Category` variable in the resumes dataset by converting it to numerical representation using `LabelEncoder` from Scikit-Learn, as these algorithms require numerical feature representations. After cleaning, I saved the text separately for topic modeling and document similarity tasks, where dataset balancing is unnecessary. 

In particular for text classification, I addressed the imbalance in underrepresented categories by employing random resampling with Scikit-Learn's `resample` method. This approach ensures that the model receives equitable representation across all categories, leading to enhanced accuracy and reduced bias, thereby improving overall performance.

## Linear SVC

::: {.column-margin}
![](/assets/projects/resume-analyzer/svc_files/svm.png)
:::

For this first model, I train a baseline Linear SVC^[Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the *maximum-margin hyperplane*, that is, the hyperplane that most clearly classifies observations] using the TF-IDF vectors. I then performance Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)^[Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.] to the TF-IDF matrix, to reduce the matrix size to a lower dimensional space. I evaluate the performance of both models, which will serve as the baseline to compare with more complex models in the upcoming sections.

> ### Takeaways
>
> -   Linear SVC achieved an accuracy of 84% on the test set.
> -   Linear SVC with Truncated SVD achieved an accuracy of 81% on the test set with only 5% of the original number of features.
> -   The baseline model achieved a high level of accuracy and is more cost-effective when reducing the dimensionality of the feature matrix.

### Import Packages and Data

Aside from standard libraries, I import two custom functions: `classifier_report` to generate a classification report and confusion matrix, and `save_performance`, which saves the performance metrics of the model to a JSON file for later analysis. I also load the pre-trained Label Encoder to label the encoded categories in upcoming plots.

```{python}
#| code-fold: false
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.decomposition import TruncatedSVD
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

from src.modules.ml import classifier_report
from src.modules.utils import save_performance

# Import the data with engineered features
df = pd.read_parquet('./data/3-processed/resume-features.parquet')

# Label model to label categories
le = joblib.load('./models/le-resumes.gz')
```

### Baseline LinearSVC

I split the dataset into 70% training and 30% testing sets. I use the `tfidf_vectors` column as the feature matrix and stack the vectors into a single matrix using `np.vstack`. For the target variable I set the encoded `Category` column. I extract the variables using the `.values` method, which extracts the variables into a NumPy array, improving performance. To verify the split, I print the shape of the training and testing sets.

```{python}
X = np.vstack(df['tfidf_vectors'].values)
y = df['Category'].values

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.30, 
                                                    random_state=42)

print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Testing set: {X_test.shape}, {y_test.shape}")
```

    Training set: (2016, 11618), (2016,)
    Testing set: (864, 11618), (864,)

Then, I train a Linear SVC model with default hyperparameters and evaluate the model's performance using the `classifier_report` function, which generates a classification report and confusion matrix.

```{python}
svc = LinearSVC(dual="auto")
accuracy = classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, True)
```

    SVC accruacy score 87.15%

![](/assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-4-output-2.png)

The model achieves an **87% accuracy** on the test set, which is already very good for a baseline model. However, TF-IDF vectors often result in sparse, high-dimensional representations with a low information ratio. To address this issue, I recur to Truncated Singular Value Decomposition (SVD).

### LinearSVC with Truncated SVD

::: {.column-margin}
![](/assets/projects/resume-analyzer/svc_files/truncated-svd.png)
:::

Transforming TF-IDF matrices by means of Truncated SVD is known as Latent Semantic Analysis (LSA). It takes the $n$ largest eigenvalues and transforms the original matrix to capture the most significant semantic relationships between terms and documents, while discarding noise and low-information features^[For an in depth explanation, see Manning, C.D., Raghavan, P. and Schütze, H. (2008) [‘Matrix decompositions and latent semantic indexing’](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf), Introduction to information retrieval, 1.].

I use `TruncatedSVD` to reduce the number of components to 500, which surprisingly is less than 5% of the original number of features. Below I apply the transformation and split the transformed feature matrix into training and testing sets before training the new model.

```{python}
t_svd = TruncatedSVD(n_components=500, algorithm='arpack')
X_svd = t_svd.fit_transform(X)


X_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(X_svd, 
                                                                    y, 
                                                                    test_size=0.30, 
                                                                    random_state=42)


print(f"Training set: {X_train_svd.shape}, {y_train_svd.shape}")
print(f"Testing set: {X_test_svd.shape}, {y_test_svd.shape}")
```

    Training set: (2016, 500), (2016,)
    Testing set: (864, 500), (864,)

I now train a new Linear SVC model using the SVD-transformed feature matrix and generate a classification report and confusion matrix.

```{python}
svc_svd = LinearSVC(dual="auto")
accuracy = classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, True)
```

    SVC accruacy score 84.94%

![](/assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-6-output-2.png)

The model achieved an **84% accuracy** on the test set, which is slightly lower than the baseline model. However, these results come from a model **trained on less than 5%** of the original number of features while still retaining a high level of accuracy. This demonstrates the sparcity of the TF-IDF vectors, while also showing that the model can still achieve an excellent level of accuracy with a substantially reduced feature matrix.

Before moving onto the next model, I save the performance metrics of the baseline model for later comparison.

```{python}
save_performance(model_name='LinearSVC',
                 architecture='default',
                 embed_size='n/a',
                 learning_rate='n/a',
                 epochs='n/a',
                 optimizer='n/a',
                 criterion='n/a',
                 accuracy=87.15
                 )
```

## Feedforward Neural Network

::: {.column-margin}
![](/assets/projects/resume-analyzer/nn_files/fnn.png)
:::

The next model is a Feedforward Neural Network (FNN)^[Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodes] built using PyTorch. I create an iterator for the dataset using the `DataLoader` class, which tokenizes and numericalized the resumes, dynamically pads the sequences, and batches the data for training, saving memory and computation time. Then, I construct a simple neural network architecture with an embedding layer, followed by three fully connected layers with ReLU activation functions. The model is trained using the Adam optimizer and CrossEntropyLoss criterion, and its performance is evaluated on the test set using accuracy as the evaluation metric.

> ### Takeaways
>
> -   The Feedforward Neural Network achieved an accuracy of 73.15% with a loss of 1.1444 on the test set.
> -   Performance suggests minimal overfitting, given the small gap between training and validation.
> -   Model demonstrates robust generalization, with test accuracy aligning closely with validation accuracy.

### Import Packages and Data

Aside from the standard PyTorch and pandas imports, I also import three custom functions:  


- `train_model`: trains the model based on the hyperparameters and data provided, prints the training and validation loss and accuracy in real-time, and saves the best model based on the iteration with the lowest validation loss. It also provides an option to visualize the training progress using the `PlotLosses` library or `matplotlib`. 
- `test_model`: evaluates the model on the test set using the best model saved during training and returns the testing accuracy.
- `save_performance`: saves the performance metrics of the model to a json file for future analysis.

```{python}
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import CrossEntropyLoss
from torch.nn.utils.rnn import pad_sequence
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, Dataset, random_split
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from livelossplot import PlotLosses
from tqdm import tqdm

from src.modules.dl import train_model, test_model
from src.modules.utils import save_performance

data = pd.read_parquet('./data/3-processed/resume-features.parquet', columns=['Category', 'cleaned_resumes'])
```

When training deep learning models, I always code the option to use a GPU if available and set the `device` variable accordingly. This not only allows the model to leverage the parallel computing if available, but also makes the code reproducible across different device setups. The snippet below checks if a GPU is available and sets the device variable accordingly, which is later used by the `DataLoader` and model.

```{python}
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device('cpu')
print("Using {}.".format(device))
```

    Using cpu.

### Dataset and DataLoader

Before constructing the `Dataset` class, I define a `tokenization` function that instantiates the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s `get_tokenizer` and `build_vocab_from_iterator` functions. The function returns the tokenized texts to be indexed by the `DataLoader` during training, and the vocabulary, which will be used to determine the vocabulary size.

```{python}
def tokenization(texts, tokenizer_type='basic_english', specials=['<unk>'], device=device):
    # Instantiate tokenizer
    tokenizer = get_tokenizer(tokenizer_type)
    # Tokenize text data
    tokens = [tokenizer(text) for text in texts]
    # Build vocabulary
    vocab = build_vocab_from_iterator(tokens, specials=specials)
    # Set default index for unknown tokens
    vocab.set_default_index(vocab['<unk>'])

    # Convert tokenized texts to a tensor
    tokenized_texts = [torch.tensor([vocab[token] for token in text], dtype=torch.int64, device=device) for text in tokens]

    return tokenized_texts, vocab
```

Next. I construct the `ResumeDataset` iterator, which preprocesses the text data using the `tokenization` function and indexes samples for the `DataLoader` during training. The `__len__` method returns the length of the dataset, the `vocab_size` method returns the size of the vocabulary, the `num_class` method returns the number of unique classes in the dataset, and the `__getitem__` method returns a sample of text and label from the dataset.

```{python}
class ResumeDataset(Dataset):
    # Dataset initialization and preprocessing
    def __init__(self, data):
        # Initialize dataset attributes
        super().__init__()
        self.text = data.iloc[:,1]
        self.labels = data.iloc[:,0]
        
        self.tokenized_texts, self.vocab = tokenization(self.text)

    # Get length of dataset
    def __len__(self):
        return len(self.labels)

    # Get vocabulary size
    def vocab_size(self):
        return len(self.vocab)

    # Get number of classes
    def num_class(self):
        return len(self.labels.unique())

    # Get item from dataset
    def __getitem__(self, idx):
        sequence = self.tokenized_texts[idx]
        label = self.labels[idx]
        return sequence, label
```

I also define a `collate_fn` function to use dynamic padding when batching the data. Dynamic padding is a technique used to pad sequences to the length of the longest sequence in a batch, as opposed to the longest sequence in the entire dataset. Because the model expects uniform dimensions to perform operations, sequences need to be padded to ensure each one has the same length. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.

```{python}
def collate_fn(batch):
    sequences, labels = zip(*batch)
    # Pad sequences to the longest sequence in the batch
    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)
    # Convert labels to tensor
    labels = torch.tensor(labels, dtype=torch.long)
    return sequences_padded, labels
```

Finally, I instantiate the `ResumeDataset` class and split the dataset into 70% training, 15% validation, and 15% test sets using the `random_split` function. I create `DataLoader` iterators for each set, using the `collate_fn` function to apply dynamic padding to the sequences.

```{python}
dataset = ResumeDataset(data)
train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
```

### Model Architecture

::: {.column-margin}
![](/assets/projects/resume-analyzer/nn_files/fnn-architecture.jpg)
:::

The model is a simple Feedforward Neural Network with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with the number of classes as the output size. The model architecture is defined in the `SimpleNN` class, which takes the vocabulary size, embedding size, number of classes as parameters. The `expansion_factor` is defined to determine the hidden dimension size, here set to 2.

The `EmbeddingBag` function efficiently computes the embeddings by performing a two-step operation: first, it creates embeddings for the input indices, adn then reduces the embedding output using the mean across the sequence dimension. This is useful for sequences of varying lengths, as it allows the model to process them more efficiently.

```{python}
class SimpleNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_class, expansion_factor=2, dropout=0.1):
        super().__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_size, sparse=False)
        self.hidden_dim = embed_size * expansion_factor
        self.layer1 = nn.Linear(embed_size, self.hidden_dim)
        self.layer2 = nn.Linear(self.hidden_dim, embed_size)
        self.layer3 = nn.Linear(embed_size, num_class)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.embedding(x)
        x = F.relu(self.layer1(x))
        x = self.dropout(x)
        x = F.relu(self.layer2(x))
        x = self.dropout(x)
        x = self.layer3(x)
        return x
```

### Hyperparameters and Training

Before training, I set the hyperparameters for training the neural network. The vocabulary size and number of classes are obtained from the `ResumeDataset` class.The embedding size is set to 60 and the learning rate is set to 1e-3. The model is trained for 40 epochs. 

```{python}
vocab_size = dataset.vocab_size()
num_class = dataset.num_class()
embed_size = 60
lr=0.001
epochs = 40
```

I then instantiate the model, sending it to the available device, and define the loss function and optimizer. The loss function is set to CrossEntropyLoss, which is suitable for multi-class classification tasks. The optimizer is Adam, an adaptive learning rate optimization algorithm well-suited for training deep neural networks. In addition, I define a learning rate scheduler that reduces the learning rate by a factor of 0.1 if the validation loss does not improve for `patience` number of epochs. This prevents the model from overfitting and improves generalization. The model and hyperparameters are then passed for training using the `train_model` function^[During fine-tunning, I found the model converged with better accuracy when using a dropout rate of 0.4.].

To visualize the training progress, I set the `visualize` parameter to 'liveloss', which uses the [PlotLosses library](https://p.migdal.pl/livelossplot/) to create a dynamicallly updating plot that visulalized the training and validation loss and accuracy in real-time. This allows me to monitor the model's performance and make adjustments to the hyperparameters if necessary.

```{python}
model = SimpleNN(vocab_size, embed_size, num_class, dropout=0.4).to(device)
criterion = CrossEntropyLoss()
loss = Adam(model.parameters(), lr=lr)
scheduler = ReduceLROnPlateau(loss, patience=2)

train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize='liveloss')
```

![](/assets/projects/resume-analyzer/nn_files/nn-plot.png)

    accuracy
      training           (min:    0.036, max:    0.712, cur:    0.707)
      validation         (min:    0.037, max:    0.694, cur:    0.685)
    log loss
      training           (min:    0.873, max:    3.187, cur:    0.921)
      validation         (min:    1.274, max:    3.184, cur:    1.330)
    ------------------------------
    Best model saved:
    Val Loss: 1.3300 | Val Acc: 0.6852
    ✅ Training complete!

Starting with the loss, the training and validation losses decrease steadily until 30 epochs, showing that the model is effectively learning the patterns in the data. After this, the training loss continues to decrease, while the validation loss plateaus. However, the model finishes with only a small gap between the training and validation loss, with the training loss at 0.92 and the validation loss at 1.33. The small size of the gap indicates that the model is not overfitting the training data and generalizes well to unseen data.

Regarding accuracy, the training and validation accuracy increase steadily until 40 epochs, after which they converge. At convergence, the training accuracy is slightly higher than the validation accuracy, with the model achieving a training accuracy of 71% and a validation accuracy of 69%. This small difference between the training and validation accuracy indicates that the model does not overfit the training data and generalizes well to unseen data. The best saved model has a validation loss of 1.33 and a validation accuracy of 68.52%.

### Evaluation

After training the model, I evaluate its performance on the test set using the `test_model` function. The function takes the trained model, test data loader, and criterion as input and returns the accuracy of the model on the test set.

```{python}
accuracy = test_model(model, test_loader, criterion)
```

    Test Loss: 1.1444 | Test Acc: 0.7315
    ✅ Testing complete!

The Feedforward Neural Network model achieves an accuracy of **73.15%** and a loss of 1.1444 on the test set. As expected from the training and validation plots, the model performs reasonably well on unseen data, with the test accuracy aligning closely with the validation accuracy observed during training. The consistency between validation and test accuracies suggests that the model successfully generalizes to new data and demonstrates robustness in its predictions. 

Compared to the baseline model, the Feedforward Neural Network model achieved a lower accuracy on the test set. However, the model’s performance is still quite impressive considering the simplicity of the architecture. The model’s performance can likely be improved by introducing more advanced regularization techniques, using pre-trained word embeddings^[Examples of pre-trained embeddings include: [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fastText](https://fasttext.cc/).], or increasing the complexity of the model architecture. This last point is what I explore in the next section, where I implement a more advanced model architecture using a Transformer-based neural network that leverages self-attention mechanisms to capture long-range dependencies in the data.

To conclude this section, I save the performance metrics of the Feedforward Neural Network model for later analysis.

```{python}
save_performance(model_name='Feedforward Neural Network',
                 architecture='embed_layer->dropout->120->dropout->60->dropout->num_classes',
                 embed_size='60',
                 learning_rate='1e-3',
                 epochs='50',
                 optimizer='Adam',
                 criterion='CrossEntropyLoss',
                 accuracy=73.15,
                 )
```

## Encoder Model

::: {.column-margin}
![](/assets/projects/resume-analyzer/encoder_files/transformer_encoder-highlighted.png)
:::

The following model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which convert dense representations into output sequences, encoders instead transform input sequences into dense representations. Their ability to extract sequence information and convert them to a dense representation makes them particularly useful for tasks such as sentiment analysis, named entity recognition, and text classification.

The encoder model follows the Transformer architecture described in *Attention is All You Need*^[Attention Is All You Need] and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and CrossEntropyLoss criterion. The model's performance is evaluated on the test set using accuracy as the evaluation metric.

The imported packages as well as the step in building the `DataLoader` are the same as those for the Feedforward Neural Network model (see [packages](https://marcocamilo.com/resume-analyzer#import-packages-and-data-1) and [data preparation](https://marcocamilo.com/resume-analyzer#dataloader). Therefore, I skip directly to the model architecture.

> ### Takeaways
>
> -   The Transformer Encoder model achieves an accuracy of 75% on the test set.
> -   This model serves as a robust baseline for transformer-based models in text classification tasks.

### Model Architecture

::: {.column-margin}
![Modified image from [Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)](/assets/projects/resume-analyzer/encoder_files/encoder.jpg)
:::

The Encoder model consists of an embedding layer, a stack of encoder layers, and a fully connected neural network for classifying the outputs. The embedding layer converts the input sequences into dense representations, which are then passed through the encoder layers. Each encoder layer consists of a multi-head self-attention mechanism[^enc-2] followed by a residual connection with layer normalization[^enc-3] and a feed-forward neural network, followed by another residual connection with layer normalization. The output is finally passed through a feed-forward neural network for classification.

I decide to build the encoder model from scratch, as it allows me to better understand the architecture and the components of the model. I opt for a modular approach, where I construct each component of the model as a separate class and then combine them in the `TransformerEncoder` class.

> **Further Reading:**
>
> The implementation of this model was in great part inspired by the following resources:
> - [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
> - [Coding a Transformer from Scratch on PyTorch (YouTube)](https://www.youtube.com/watch?v=ISNdQcPhsts)
> - [Text Classification with Transformer Encoders](https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae)

```{python}
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size: int, d_model: int):
        super().__init__()
        # Dimensions of embedding layer
        self.embedding = nn.Embedding(vocab_size, d_model)
        # Embedding dimension
        self.d_model = d_model

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEmbedding(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        # Initialize positional embedding matrix (vocab_size, d_model)
        pe = torch.zeros(vocab_size, d_model)
        # Positional vector (vocab_size, 1)
        position = torch.arange(0, vocab_size).unsqueeze(1)
        # Frequency term
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))
        # Sinusoidal functions
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # Add batch dimension
        pe = pe.unsqueeze(0)
        # Save to class
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class LayerNorm(nn.Module):
    def __init__(self, d_model: int, eps: float = 1e-6):
        super().__init__()
        # Learnable parameters
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.ones(d_model))
        # Numerical stability in case of 0 denominator
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        # Linear combination of layer norm with parameters gamma and beta
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

class ResidualConnection(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1):
        super().__init__()
        # Layer normalization for residual connection
        self.norm = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x1, x2):
        return self.dropout(self.norm(x1 + x2))

class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):
        super().__init__()
        # Linear layers and dropout
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, dropout: float =0.1, qkv_bias: bool = False, is_causal: bool = False):
        super().__init__()
        assert d_model % num_heads == 0,  "d_model is not divisible by num_heads"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.dropout = dropout
        self.is_causal = is_causal

        self.qkv = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)
        self.linear = nn.Linear(num_heads * self.head_dim, d_model)
        self.dropout_layer = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        batch_size, seq_length = x.shape[:2]

        # Linear transformation and split into query, key, and value
        qkv = self.qkv(x)  # (batch_size, seq_length, 3 * embed_dim)
        qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)  # (batch_size, seq_length, 3, num_heads, head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_length, head_dim)
        queries, keys, values = qkv  # 3 * (batch_size, num_heads, seq_length, head_dim)

        # Scaled Dot-Product Attention
        context_vec = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask, dropout_p=self.dropout, is_causal=self.is_causal)

        # Combine heads, where self.d_model = self.num_heads * self.head_dim
        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
        context_vec = self.dropout_layer(self.linear(context_vec))

        return context_vec

class EncoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        # Multi-head self-attention mechanism
        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)
        # First residual connection and layer normalization
        self.residual1 = ResidualConnection(d_model, dropout)
        # Feed-forward neural network
        self.feed_forward = FeedForward(d_model, hidden_dim, dropout)
        # Second residual connection and layer normalization
        self.residual2 = ResidualConnection(d_model, dropout)

    def forward(self, x, mask=None):
        x = self.residual1(x, self.multihead_attention(x, mask))
        x = self.residual2(x, self.feed_forward(x))
        return x

class EncoderStack(nn.Module):
    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, dropout: float = 0.1):
        super().__init__()
        # Stack of encoder layers
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)])

    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask)
        return x

class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, out_features: int, dropout: float = 0.1):
        super().__init__()
        self.embedding = EmbeddingLayer(vocab_size, d_model)
        self.positional_embedding = PositionalEmbedding(vocab_size, d_model, dropout)
        self.encoder = EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)
        self.classifier = nn.Linear(d_model, out_features)

    def forward(self, x, mask=None):
        x = self.embedding(x)
        x = self.positional_embedding(x)
        x = self.encoder(x, mask)
        x = x.mean(dim=1)
        x = self.classifier(x)
        return x
```

### Hyperparameters and Training

With the model constructed, I initialize the hyperparameters for training. As with the previous model, I obtain the vocabulary size and number of output features from the `ResumeDataset` class. The embedding size is fixed at 80, while the hidden dimension is set to 180. For the multi-head attention mechanism, I use 4 heads, with an the encoder stack comprising of 4 layers. I train the model for 20 epochs at a learning rate of 1e-3.


```{python}
vocab_size = dataset.vocab_size()
d_model = 80
num_heads = 4
hidden_dim = 180
num_layers = 4
out_features = dataset.num_class()
lr = 0.001
epochs = 20
```

I instantiate the model with the hyperparameters and move it to the device. The criterion and optimizer are left unchanged from the previous model, with the optimizer set to the Adam optimizer and the criterion set to the CrossEntropyLoss, suitable for multi-class classification tasks. I also initialize the learning rate scheduler with a patience of 2, to prevent the model from overfitting. The model is then trained using the `train_model` function.

```{python}
model = TransformerClassifier(vocab_size, d_model, num_heads, 
                                hidden_dim, num_layers, out_features, dropout=0).to(device)
criterion = CrossEntropyLoss()
loss = Adam(model.parameters(), lr=lr)
scheduler = ReduceLROnPlateau(loss, patience=2)

train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)
```

![](/assets/projects/resume-analyzer/encoder_files/encoder-plot.png)

    accuracy
      training         	 (min:    0.043, max:    1.000, cur:    1.000)
      validation       	 (min:    0.035, max:    0.794, cur:    0.785)
    log loss
      training         	 (min:    0.027, max:    3.276, cur:    0.029)
      validation       	 (min:    1.023, max:    3.273, cur:    1.071)
    ------------------------------
    Best model saved:
    Val Loss: 1.0709 | Val Acc: 0.7847
    ✅ Training complete!

According to the performance plots, despite the encoder model achieving a better validation accuracy than the Feedforward Neural Network, the model exhibits a large gap between the training and validation performance, indicating that the model is overfitting. The model rapidly decreases the training and validation losses, and rapidly incrases their accuracies during the first 14 epochs. For the following 6 epochs, however, the training loss continues to rapidly decrease from 1.25 to near 0, while the validation loss stagnates at around 1.07. Similarly, the training accuracy rapidly increases to 100%, while the validation accuracy remains at 78%. The model converges 20 epochs in with a significant gap between training and validation models, indicating that the model is overfitting.

Nevertheless, the model achieves a validation accuracy of 78%, which is slightly higher than the Feedforward Neural Network model. I decide to evaluate the model on the test set to obtain the final accuracy.

### Evaluation

```{python}
accuracy = test_model(model, test_loader, criterion)
```

    Test Loss: 1.2526 | Test Acc: 0.7454
    ✅ Testing complete!

Despite having implemented a more advanced model architecture with the addition of the multi-head self-attention mechanism, the encoder model achieves an accuracy of 74.5%, similar to the Feedforward Neural Network model. As discussed earlier, the model exhibits a large gap between the training and validation performance, which could be hindering the model's generalization capabilities. 

Given a better training and validation performance, the model could potentially achieve a higher accuracy on the test set. The model's performance could likely be improved by means of data augmentation, modifying hyperparameters such as embedding size, hidden dimension, and number of layers, or by using a different optimizer or learning rate scheduler.

As with the previous models, I save the performance metrics for later analysis.


```{python}
save_performance(model_name='Transformer',
                 architecture='embed_layer->encoder->linear_layer',
                 embed_size='64',
                 learning_rate='1e-3',
                 epochs='20',
                 optimizer='Adam',
                 criterion='CrossEntropyLoss',
                 accuracy=80
                 )
```

[^enc-2]: The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.

[^enc-3]: Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.

## BERT

::: {.column-margin}
![](/assets/projects/resume-analyzer/bert_files/bert-classifier.png)
:::

The last model is also an encoder-based model called the Bidirectional Encoder Representations from Transformers (BERT). BERT is a pre-trained transformer-based model that can be fine-tuned for a wide range of NLP tasks by adding task-specific output layers. The model is bidirectional, meaning that it can take into account the context of a word by looking at both the left and right context. This allows the model to capture a wider range of contextual information, which is particularly useful for tasks such as text classification. This allows the model to capture rich semantic relationships and dependencies within text sequences, which is particularly useful for tasks such as text classification.

As with previous PyTorch models, I create an iterable dataset using the `Dataset` and `DataLoader` classes, to tokenize the resumes using the BERT tokenizer, pad sequences to equal lengths, split the data and batch the data for training. I then construct the model architecture, consisting of the pre-trained BERT base model, an added dropout layer and a linear output layer for classification. I initialize the hyperparameters and train the model using Cross Entropy Loss along with the Adam optimizer. The model is evaluated as with other deep learning models using accuracy.

> ### Takeaways
> -
> -
> -

### Import Packages

In addition to the standard deep learning packages used so far, I import three classes from the `transformers` package:

-   `BertModel`: loads the pre-trained BERT model.
-   `BertTokenizer`: constructs a BERT tokenizer.
-   `DataCollatorWithPadding`: builds a batch with dynamically padded sequences.

Because of BERT's output format, I also import a custom `train_BERT` and `test_BERT` function, which are specifically tailored to return the model's training and test performances using BERT's output, including input IDs and attention masks.


```{python}
from transformers import BertModel, BertTokenizer, DataCollatorWithPadding

from src.modules.dl import train_BERT, test_BERT
```

### Dataset and DataLoader

Before creating the dataset and dataloader, I initizalize the tokenizer and define the pre-trained BERT model. I then create the `ResumeBertDataset`, which tokenizes resumes and prepares them for model input.

In contrast to the previous model, I configure the tokenizer using the `.encode_plus` method, which returns a dictionary of the batch encodings, including tokenized input sequences and attention masks. I set the `padding` parameter to `False` to avoid padding, as this will be handled dynamically by the data collator. Additionally, I set `truncation` to `True` to truncate sequences that exceed the maximum length. The method also adds the special tokens `[CLS]` and `[SEP]` to the input sequences, required by BERT. I set the `return_tensors` parameter to `'pt'` to return PyTorch tensors. Finally, I return a dictionary with the processed input sequences, attention masks, and labels.


```{python}
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class ResumeBertDataset(Dataset):
    def __init__(self, data, max_length, tokenizer=tokenizer, device=device):
        super().__init__()
        self.texts = data.iloc[:,1].values
        self.labels = torch.tensor(data.iloc[:,0])
        self.max_length = max_length
        self.tokenizer = tokenizer
        self.device = device

    def __len__(self):
        return len(self.labels)

    def num_class(self):
        return len(self.labels.unique())

    def __getitem__(self, idx):
        resumes = self.texts[idx]
        labels = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            resumes,
            add_special_tokens=True,
            max_length=self.max_length,
            truncation=True,
            padding=False,
            return_attention_mask=True,
            return_tensors='pt'
        ).to(self.device)

        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }
```

I initialize the dataset and set `max_length` to 512, which is the maximum number of tokens that BERT can process. I then split the dataset into 70% training, 15% validation, and 15% test sets using the `random_split` function. I use the `DataCollatorWithPadding` class to dynamically pad sequences to the maximum length in each batch. Finally, I create dataloaders for the training, validation, and test sets using the `DataLoader` class, setting the batch size to 16, shuffling the data, and assigning the data collator.


```{python}
dataset = ResumeBertDataset(data, max_length=512)
train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)
```

### Model Architecture

The `BertResumeClassifier` model consists of the pre-trained BERT base model, a dropout layer, and a linear output layer to classify the resumes. The BERT model uses the `bert-base-uncased` pre-trained model to generate contextual embeddings from the input sequences. I extract the embeddings by indexing the `pooler_output` key from the output dictionary. These embeddings are then passed through a dropout layer to prevent overfitting, and subsequently fed into a fully connected linear layer which maps the embeddings to the desired number of output classes for classification.


```{python}
class BertResumeClassifier(nn.Module):
    def __init__(self, n_classes: int, dropout: float = 0.01):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
          input_ids=input_ids,
          attention_mask=attention_mask
        )['pooler_output']
        output = self.dropout(pooled_output)
        output = self.out(output)
        return output
```

### Hyperparameters and Training

Since BERT is a pre-trained model and it accepts a fixed input size of 512 tokens, the are less parameters to set from the model itself. The only parameter that needs to be set is the number of classes---which, as before, is obtained from the `Dataset` class.

I initialize the model, loss function, optimizer, and number of epochs. I use as before the Cross Entropy Loss function and the Adam optimizer, although this time with a learning rate of 2e-5, since it seemed to result in better performance. I train the model for only 10 epochs.

```{python}
n_classes = dataset.num_class()

model = BertResumeClassifier(n_classes).to(device)
criterion = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=2e-5)
epochs = 10

train_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)
```

![](/assets/projects/resume-analyzer/bert_files/bert-plot.png)

    accuracy
        training         	 (min:    0.050, max:    0.991, cur:    0.991)
        validation       	 (min:    0.120, max:    0.933, cur:    0.933)
    log loss
        training         	 (min:    0.081, max:    3.189, cur:    0.081)
        validation       	 (min:    0.414, max:    3.002, cur:    0.428)
    ------------------------------
    Best model saved:
    Val Loss: 0.4143 | Val Acc: 0.9190
    ✅ Training complete!

The BERT model shows a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the model's train loss decreased from 3.1394 to 0.0589, while the validation loss decreased from 2.7347 to 0.5070. This consistent reduction highlights the model's ability to capture and generalize the data without overfitting, as demonstrated by the small gap between the training and validation losses by the end of the 10th epoch.

Regarding accuracy, the training and validation accuracies also show a steady increase over the 10 epochs. The training accuracy increased from 0.0774 to 0.9950, with validation accuracy improving from 0.3472 to 0.9028. This indicates that the model effectively learned the patterns in the data and generalizes well to unseen data. The small difference between the final training and validation accuracies demonstrates the model's robustness and ability to avoid overfitting, ensuring reliable performance on new data. The best saved model has a validation loss of 0.5070 and an accuracy of 0.9028.

### Evaluation

As before, I evaluate the model using the `test_model` function using the best saved model.

```{python}
accuracy = test_BERT(model, test_loader, criterion)
```

    Test Loss: 0.3984 | Test Acc: 0.9167
    ✅ Testing complete!

The BERT model achieved an impressive performance on the test set, reaching an accuracy of 91.67% with a test loss of 0.3984. This significantly outperforms all previous models tested so far. As expected from the training and validation performances, the model is robust and generalizes very well to unseen data. This is further confirmed by the very close alignement between test and validation accuracies, both of which represent datasets not previously seen by the model. The close accuracy between the validation data and the test data shows the model is capable to generalize to new resumes and effectively classify them into the correct categories.

As with previous models, I save its performance using the `save_performance` function.

```{python}
save_performance(model_name='BERT',
                 architecture='bert-base-uncased>dropout->linear_layer',
                 embed_size='768',
                 learning_rate='2e-5',
                 epochs='10',
                 optimizer='Adam',
                 criterion='CrossEntropyLoss',
                 accuracy=90
                 )
```

## Results and Discussion

In this section of the project, I explored four different models for resume classification: Linear SVC, FNN, Transformer, and BERT. I evaluated the performance of each model using the accuracy metric and collected the results after each deployment. Below I plot the accuracy of each model and discuss the results.

```{python}
evaluation_df = pd.read_json('./output/classifier_performance.json').sort_values(by='accuracy', ascending=False)

ax = sns.barplot(evaluation_df, x='model', y='accuracy', hue='model', palette='hls')
[ax.bar_label(container, fmt="%0.2f%%") for container in ax.containers]
plt.show()
```

::: {.column-margin}
![](/assets/projects/resume-analyzer/bert_files/bert-plot.png)
:::

All four models performed well in classifying resumes, achieving accuracies above 70%. This success can be largely attributed to the effectiveness of the data preparation process, including text preprocessing, data balancing, and robust vectorization techniques. These preprocessing steps provided the models with high-quality input features that significantly enhanced their performance.

Upon closer evaluation, the models can be grouped into two categories based on their performance. The first group, achieving around 90% accuracy, includes the Linear SVC and BERT models. The second group, with accuracies around 70%, includes the FNN and Transformer models. Interestingly, the two best-performing models feature both the simplest and most complex architectures respectively, while the models with the lowest performance have more complex architectures than the baseline model. I discuss the reasons behind these results below.

Linear SVC's high performance can be attributed its simplicity and the effective feature representation^[To read more on the efficiency of linear classifiers in text classification, see [Lin, Y.-C. et al. (2023) 'Linear Classifier: An Often-Forgotten Baseline for Text Classification'](https://doi.org/10.18653/v1/2023.acl-short.160).]. The model is a classical machine learning algorithm that uses a linear kernel and no deep learning, which yields a simple architecture that is easy to train. Additionally, the model was trained on TF-IDF vectors, which result in a matrix with simple, but interpretable and informative features. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model's high accuracy and fast training times.

In contrast, BERT's performance strems from its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information^[See [Devlin, J. et al. (2018) 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'](https://doi.org/10.48550/ARXIV.1810.04805).]. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Its suitability for Transfer Learning allows maximizing its pre-trained weights and easily fine-tuning on resume classification task, resulting in the high accuracy achieved.

The FNN and Transformer models, despite their increased complexity, achieved lower accuracies of around 70%. The Feedforward Neural Network, while more advanced than a linear model, lacks the ability to capture sequential dependencies and contextual nuances in the data, thus expected to perform worse than the transformer models.

However, the Transformer model should have been able to capture the sequential dependencies in the resume data, given its state-of-the-art architecture and use of multi-head self-attention. But contrary to BERT, the Transformer used in this project was not pre-trained on a large corpus. This limited its ability to generate high-quality dense representations of the texts. Additionally, insufficient fine-tuning may have prevented the Transformer from reaching its full potential. Given the results of a similar, more complex model such as BERT, additional hyperparameter tuning and training time could improve its performance.

## Conclusion

In this project, I explored the task of resume classification using machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented four models: a Linear Support Vector Classifier, a Feedforward Neural Network, a Transformer model, and a BERT model. These models were trained and evaluated on the resume dataset, with performance compared based on accuracy.

The results were very insightful and provoked interesting observations on the role of model complexity and feature representation in achieving high performance. The BERT model achieved the highest accuracy of 91.67%, closely followed by the LinearSVC model at 87.15%. The Feedforward Neural Network and Transformer models achieved lower accuracies of 73.15% and 74.54%, respectively. BERT's superior performance can be attributed to its pre-trained transformer architecture, which captures rich semantic relationships and dependencies within text sequences. The strong performance of the LinearSVC model can be attributed to its simplicity and efficiency in handling high-dimensional data, leveraging high-quality, interpretable feature representations such as TF-IDF vectors.

Two important observations arise from these results:

>
> 1. State-of-the-art transformer models, combined with transfer learning from pre-trained models like BERT, yield the best performance.
> 2. Simple models with high-quality, interpretable feature representations such as TF-IDF vectors, can also achieve high performance.

These contrasting observations indicate that model complexity alone does not guarantee high performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline. 

> **The quality of the feature representation and the ability to capture contextual information across dependencies are more important factors in achieving high performance.**

Ultimately, the best model depends on the requirements of the task at hand and the resources available for development. For tasks where high performance is critical and ample resources are available, using state-of-the-art transformer models such as BERT with transfer learning are recommended. For tasks prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors still offers a high-performance, resource-efficient alternative.

